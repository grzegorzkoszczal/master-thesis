% Tutorial:
% https://www.overleaf.com/learn/latex/Bibliography_management_with_biblatex
% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#biblatex
% For custom sorting, use `key` parameter. By default, positions are
% sorted according to the `author` field.

@misc{book:latex,
  title    = {LaTeX},
  subtitle = {Guide to the LaTeX typesetting system},
  keywords = {Useful_Link}
}
    % {} are used in 'author' field to disable printing the author 'Joe
    % Doe' name, as 'J. Doe'. Overall, you can use them to enforce
    % verbatim representation.
    author       = "{Wikibooks community}",
    publisher    = "Wikibooks",
    howpublished = "\url{https://en.wikibooks.org/wiki/LaTeX}",


# ===== RESEARCH PAPERS =====

# One of the major articles this Master Thesis is based on
# It covers the majority of the main goal set in this work: Measurements of energy used
# during execution of device straining benchmarks and comparison of Intel RAPL and NVIDIA NVML
# measurements results with the external power meters.
# Used in "state-of-the-art" chapter, as the first article mentioned
@article{State_of_the_Art_Article_1,
  author         = {Fahad, Muhammad and Shahid, Arsalan and Manumachu, Ravi Reddy and Lastovetsky, Alexey},
  title          = {A Comparative Study of Methods for Measurement of Energy of Computing},
  journal        = {Energies},
  volume         = {12},
  year           = {2019},
  number         = {11},
  article-number = {2204},
  url            = {https://www.mdpi.com/1996-1073/12/11/2204},
  issn           = {1996-1073},
  abstract       = {Energy of computing is a serious environmental concern and mitigating it is an important technological challenge. Accurate measurement of energy consumption during an application execution is key to application-level energy minimization techniques. There are three popular approaches to providing it: (a) System-level physical measurements using external power meters; (b) Measurements using on-chip power sensors and (c) Energy predictive models. In this work, we present a comprehensive study comparing the accuracy of state-of-the-art on-chip power sensors and energy predictive models against system-level physical measurements using external power meters, which we consider to be the ground truth. We show that the average error of the dynamic energy profiles obtained using on-chip power sensors can be as high as 73\% and the maximum reaches 300\% for two scientific applications, matrix-matrix multiplication and 2D fast Fourier transform for a wide range of problem sizes. The applications are executed on three modern Intel multicore CPUs, two Nvidia GPUs and an Intel Xeon Phi accelerator. The average error of the energy predictive models employing performance monitoring counters (PMCs) as predictor variables can be as high as 32\% and the maximum reaches 100\% for a diverse set of seventeen benchmarks executed on two Intel multicore CPUs (one Haswell and the other Skylake). We also demonstrate that using inaccurate energy measurements provided by on-chip sensors for dynamic energy optimization can result in significant energy losses up to 84\%. We show that, owing to the nature of the deviations of the energy measurements provided by on-chip sensors from the ground truth, calibration can not improve the accuracy of the on-chip sensors to an extent that can allow them to be used in optimization of applications for dynamic energy. Finally, we present the lessons learned, our recommendations for the use of on-chip sensors and energy predictive models and future directions.},
  keywords       = {Research_Paper},
  doi            = {10.3390/en12112204}
}

# Second major article, being a backbone of this Master Thesis
# Used in "state-of-the-art" chapter, as the second article mentioned
@inproceedings{State_of_the_Art_Article_2,
  author    = {Arafa, Yehia and ElWazir, Ammar and ElKanishy, Abdelrahman and Aly, Youssef and Elsayed, Ayatelrahman and Badawy, Abdel-Hameed and Chennupati, Gopinath and Eidenbenz, Stephan and Santhi, Nandakishore},
  title     = {Verified Instruction-Level Energy Consumption Measurement for NVIDIA GPUs},
  year      = {2020},
  isbn      = {9781450379564},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3387902.3392613},
  doi       = {10.1145/3387902.3392613},
  abstract  = {GPUs are prevalent in modern computing systems at all scales. They consume a significant fraction of the energy in these systems. However, vendors do not publish the actual cost of the power/energy overhead of their internal microarchitecture. In this paper, we accurately measure the energy consumption of various PTX instructions found in modern NVIDIA GPUs. We provide an exhaustive comparison of more than 40 instructions for four high-end NVIDIA GPUs from four different generations (Maxwell, Pascal, Volta, and Turing). Furthermore, we show the effect of the CUDA compiler optimizations on the energy consumption of each instruction. We use three different software techniques to read the GPU on-chip power sensors, which use NVIDIA's NVML API and provide an in-depth comparison between these techniques. Additionally, we verified the software measurement techniques against a custom-designed hardware power measurement. The results show that Volta GPUs have the best energy efficiency of all the other generations for the different categories of the instructions. This work should aid in understanding NVIDIA GPUs' microarchitecture. It should also make energy measurements of any GPU kernel both efficient and accurate.},
  booktitle = {Proceedings of the 17th ACM International Conference on Computing Frontiers},
  pages     = {60-70},
  numpages  = {11},
  keywords  = {Research_Paper, PTX, NVML, internal power sensors, external power meters, GPU power usage, PAPI},
  location  = {Catania, Sicily, Italy},
  series    = {CF '20}
}

# Third major article
# Used in "state-of-the-art" chapter, as the third article mentioned
@inproceedings{State_of_the_Art_Article_3,
  author    = {Burtscher, Martin and Zecena, Ivan and Zong, Ziliang},
  title     = {Measuring GPU Power with the K20 Built-in Sensor},
  year      = {2014},
  isbn      = {9781450327664},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2588768.2576783},
  doi       = {10.1145/2588768.2576783},
  abstract  = {GPU-accelerated programs are becoming increasingly common in HPC, personal computers, and even handheld devices, making it important to optimize their energy efficiency. However, accurately profiling the power consumption of GPU code is not straightforward. In fact, we have identified multiple anomalies when using the on-board power sensor of K20 GPUs. For example, we have found that doubling a kernel's runtime more than doubles its energy usage, that kernels consume energy after they have stopped executing, and that running two kernels in close temporal proximity inflates the energy consumption of the later kernel. Moreover, we have observed that the power sampling frequency varies greatly and that the GPU sensor only performs power readings once in a while. We present a methodology to accurately compute the instant power and the energy consumption despite these issues.},
  booktitle = {Proceedings of Workshop on General Purpose Processing Using GPUs},
  pages     = {28-36},
  numpages  = {9},
  keywords  = {Research_Paper, Power measurement, GPU power sensor, energy measurement},
  location  = {Salt Lake City, UT, USA},
  series    = {GPGPU-7}
}

# NAS Parallel Benchmarks Article
# 
@inbook{NPB,
  author    = {Bailey, David H.},
  editor    = {Padua, David},
  title     = {NAS Parallel Benchmarks},
  booktitle = {Encyclopedia of Parallel Computing},
  year      = {2011},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {1254--1259},
  isbn      = {978-0-387-09766-4},
  doi       = {10.1007/978-0-387-09766-4_133},
  keywords  = {Research_Paper},
  url       = {https://doi.org/10.1007/978-0-387-09766-4_133}
}

# NPB-CPP Article
# Used in section explaining benchmarks
@article{NPB-CPP,
  title    = {The NAS Parallel Benchmarks for evaluating C++ parallel programming frameworks on shared-memory architectures},
  journal  = {Future Generation Computer Systems},
  volume   = {125},
  pages    = {743-757},
  year     = {2021},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2021.07.021},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X21002831},
  author   = {Júnior Löff and Dalvan Griebler and Gabriele Mencagli and Gabriell Araujo and Massimo Torquati and Marco Danelutto and Luiz Gustavo Fernandes},
  keywords = {Research_Paper, NAS Parallel Benchmarks, Parallel programming, Multicore architectures, Performance evaluation},
  abstract = {The NAS Parallel Benchmarks (NPB), originally implemented mostly in Fortran, is a consolidated suite containing several benchmarks extracted from Computational Fluid Dynamics (CFD) models. The benchmark suite has important characteristics such as intensive memory communications, complex data dependencies, different memory access patterns, and hardware components/sub-systems overload. Parallel programming APIs, libraries, and frameworks that are written in C++ as well as new optimizations and parallel processing techniques can benefit if NPB is made fully available in this programming language. In this paper we present NPB-CPP, a fully C++ translated version of NPB consisting of all the NPB kernels and pseudo-applications developed using OpenMP, Intel TBB, and FastFlow parallel frameworks for multicores. The design of NPB-CPP leverages the Structured Parallel Programming methodology (essentially based on parallel design patterns). We show the structure of each benchmark application in terms of composition of few patterns (notably Map and MapReduce constructs) provided by the selected C++ frameworks. The experimental evaluation shows the accuracy of NPB-CPP with respect to the original NPB source code. Furthermore, we carefully evaluate the parallel performance on three multi-core systems (Intel, IBM Power, and AMD) with different C++ compilers (gcc, icc, and clang) by discussing the performance differences in order to give to the researchers useful insights to choose the best parallel programming framework for a given type of problem.}
}

# NPB-CUDA Article #1
# Used in section explaining benchmarks
# https://doi.org/10.1002/spe.3056
# https://onlinelibrary.wiley.com/doi/10.1002/spe.3056#pane-pcw-references
@article{NPB-CUDA_1,
  author   = {Araujo, Gabriell and Griebler, Dalvan and Rockenbach, Dinei A. and Danelutto, Marco and Fernandes, Luiz G.},
  title    = {NAS Parallel Benchmarks with CUDA and beyond},
  journal  = {Software: Practice and Experience},
  volume   = {53},
  number   = {1},
  pages    = {53-80},
  keywords = {Research_Paper, graphics processing units, high-performance computing, NPB, parallel applications, parallel programming, performance analysis},
  doi      = {https://doi.org/10.1002/spe.3056},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3056},
  abstract = {Abstract NAS Parallel Benchmarks (NPB) is a standard benchmark suite used in the evaluation of parallel hardware and software. Several research efforts from academia have made these benchmarks available with different parallel programming models beyond the original versions with OpenMP and MPI. This work joins these research efforts by providing a new CUDA implementation for NPB. Our contribution covers different aspects beyond the implementation. First, we define design principles based on the best programming practices for GPUs and apply them to each benchmark using CUDA. Second, we provide ease of use parametrization support for configuring the number of threads per block in our version. Third, we conduct a broad study on the impact of the number of threads per block in the benchmarks. Fourth, we propose and evaluate five strategies for helping to find a better number of threads per block configuration. The results have revealed relevant performance improvement solely by changing the number of threads per block, showing performance improvements from 8\% up to 717\% among the benchmarks. Fifth, we conduct a comparative analysis with the literature, evaluating performance, memory consumption, code refactoring required, and parallelism implementations. The performance results have shown up to 267\% improvements over the best benchmarks versions available. We also observe the best and worst design choices, concerning code size and the performance trade-off. Lastly, we highlight the challenges of implementing parallel CFD applications for GPUs and how the computations impact the GPU's behavior.},
  year     = {2023}
}

# NPB-CUDA Article #2
# Used in section explaining benchmarks
# https://ieeexplore.ieee.org/document/9092341
@inproceedings{NPB-CUDA_2,
  author    = {Araujo, Gabriell Alves de and Griebler, Dalvan and Danelutto, Marco and Fernandes, Luiz Gustavo},
  booktitle = {2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
  title     = {Efficient NAS Parallel Benchmark Kernels with CUDA},
  keywords  = {Research_Paper},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {9-16},
  doi       = {10.1109/PDP50117.2020.00009}
}

# Article about increasing energy consumption by data centers
# Used in introduction
@article{Data_Centre_Energy_Consumption,
  author         = {Avgerinou, Maria and Bertoldi, Paolo and Castellazzi, Luca},
  title          = {Trends in Data Centre Energy Consumption under the European Code of Conduct for Data Centre Energy Efficiency},
  keywords       = {Research_Paper},
  journal        = {Energies},
  volume         = {10},
  year           = {2017},
  number         = {10},
  article-number = {1470},
  url            = {https://www.mdpi.com/1996-1073/10/10/1470},
  issn           = {1996-1073},
  doi            = {10.3390/en10101470}
}

# Article about using the GPU Power Capping for energy-aware computing
# Currently NOT implemented
@inproceedings{GPU_Power_Capping_Czarnul,
  author    = {Krzywaniak, Adam
               and Czarnul, Pawel
               and Proficz, Jerzy},
  editor    = {Groen, Derek
               and de Mulatier, Cl{\'e}lia
               and Paszynski, Maciej
               and Krzhizhanovskaya, Valeria V.
               and Dongarra, Jack J.
               and Sloot, Peter M. A.},
  title     = {GPU Power Capping for Energy-Performance Trade-Offs in Training of Deep Convolutional Neural Networks for Image Recognition},
  keywords  = {Research_Paper},
  booktitle = {Computational Science -- ICCS 2022},
  year      = {2022},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {667--681},
  abstract  = {In the paper we present performance-energy trade-off investigation of training Deep Convolutional Neural Networks for image recognition. Several representative and widely adopted network models, such as Alexnet, VGG-19, Inception V3, Inception V4, Resnet50 and Resnet152 were tested using systems with Nvidia Quadro RTX 6000 as well as Nvidia V100 GPUs. Using GPU power capping we found other than default configurations minimizing three various metrics: energy (E), energy-delay product (EDP) as well as energy-delay sum (EDS) which resulted in considerable energy savings, with a low to medium performance loss for EDP and EDS. Specifically, for Quadro 6000 and minimization of E we obtained energy savings of 28.5{\%}--32.5{\%}, for EDP 25{\%}--28{\%} of energy was saved with average 4.5{\%}--15.4{\%} performance loss, for EDS (k = 2) 22{\%}--27{\%} of energy was saved with 4.5{\%}--13.8{\%} performance loss. For V100 we found average energy savings of 24{\%}--33{\%}, for EDP energy savings of 23{\%}--27{\%} with corresponding performance loss of 13{\%}--21{\%} and for EDS (k = 2) 23.5{\%}--27.3{\%} of energy was saved with performance loss of 4.5{\%}--13.8{\%}.},
  isbn      = {978-3-031-08751-6}
}

# Article about using the Power Capping in order to analyze energy/performance results in benchmarks
# Used in Introduction
# https://www.researchgate.net/publication/327893865_Analyzing_energyperformance_trade-offs_with_power_capping_for_parallel_applications_on_modern_multi_and_many_core_processors
@inproceedings{Parallel_Apps_Power_Capping_Czarnul,
  author   = {Krzywaniak, Adam and Proficz, Jerzy and Czarnul, Pawel},
  title    = {Analyzing energy/performance trade-offs with power capping for parallel applications on modern multi and many core processors},
  keywords = {Research_Paper},
  year     = {2018},
  month    = {09},
  pages    = {339-346},
  doi      = {10.15439/2018F177}
}

# Article about improving the energy efficiency of computing
# Used in Introduction
@article{Software_Methods_of_Energy_Efficiency,
  author   = {Chao Jin and Bronis R de Supinski and David Abramson and Heidi Poxon and Luiz DeRose and Minh Ngoc Dinh and Mark Endrei and Elizabeth R Jessup},
  title    = {A survey on software methods to improve the energy efficiency of parallel computing},
  keywords = {Research_Paper},
  journal  = {The International Journal of High Performance Computing Applications},
  volume   = {31},
  number   = {6},
  pages    = {517-549},
  year     = {2017},
  doi      = {10.1177/1094342016665471},
  abstract = {Energy consumption is one of the top challenges for achieving the next generation of supercomputing. Codesign of hardware and software is critical for improving energy efficiency (EE) for future large-scale systems. Many architectural power-saving techniques have been developed, and most hardware components are approaching physical limits. Accordingly, parallel computing software, including both applications and systems, should exploit power-saving hardware innovations and manage efficient energy use. In addition, new power-aware parallel computing methods are essential to decrease energy usage further. This article surveys software-based methods that aim to improve EE for parallel computing. It reviews the methods that exploit the characteristics of parallel scientific applications, including load imbalance and mixed precision of floating-point (FP) calculations, to improve EE. In addition, this article summarizes widely used methods to improve power usage at different granularities, such as the whole system and per application. In particular, it describes the most important techniques to measure and to achieve energy-efficient usage of various parallel computing facilities, including processors, memories, and networks. Overall, this article reviews the state-of-the-art of energy-efficient methods for parallel computing to motivate researchers to achieve optimal parallel computing under a power budget constraint.}
}

@article{Power_Management_on_Intel_Microprocessor,
  author   = {Rotem, Efraim and Naveh, Alon and Ananthakrishnan, Avinash and Weissmann, Eliezer and Rajwan, Doron},
  journal  = {IEEE Micro},
  title    = {Power-Management Architecture of the Intel Microarchitecture Code-Named Sandy Bridge},
  year     = {2012},
  volume   = {32},
  number   = {2},
  pages    = {20-27},
  abstract = {Modern microprocessors are evolving into system-on-a-chip designs with high integration levels, catering to ever-shrinking form factors. Portability without compromising performance is a driving market need. An architectural approach that's adaptive to and cognizant of workload behavior and platform physical constraints is indispensable to meeting these performance and efficiency goals. This article describes power-management innovations introduced on Intel's Sandy Bridge microprocessor.},
  keywords = {Research_Paper},
  doi      = {10.1109/MM.2012.12},
  issn     = {1937-4143},
  month    = {03}
}

@inproceedings{RAPL_Power_Estimation_and_Capping,
  author    = {David, Howard and Gorbatov, Eugene and Hanebutte, Ulf R. and Khanna, Rahul and Le, Christian},
  booktitle = {2010 ACM/IEEE International Symposium on Low-Power Electronics and Design (ISLPED)},
  title     = {RAPL: Memory power estimation and capping},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {189-194},
  abstract  = {The drive for higher performance and energy efficiency in data-centers has influenced trends toward increased power and cooling requirements in the facilities. Since enterprise servers rarely operate at their peak capacity, efficient power capping is deemed as a critical component of modern enterprise computing environments. In this paper we propose a new power measurement and power limiting architecture for main memory. Specifically, we describe a new approach for measuring memory power and demonstrate its applicability to a novel power limiting algorithm. We implement and evaluate our approach in the modern servers and show that we achieve up to 40\% lower performance impact when compared to the state-of-art baseline across the power limiting range.},
  keywords  = {Research_Paper},
  doi       = {10.1145/1840845.1840883},
  issn      = {},
  month     = {08}
}

# [I need to check exact contents of this article and it's
# usefulness as a reference]
# Used in "state-of-the-art" chapter, first article
@article{Implementation_for_Accelerator_Kernels,
  author     = {Khaleghzadeh, Hamidreza and Zhong, Ziming and Reddy, Ravi and Lastovetsky, Alexey},
  title      = {Out-of-Core Implementation for Accelerator Kernels on Heterogeneous Clouds},
  year       = {2018},
  issue_date = {February  2018},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {74},
  number     = {2},
  issn       = {0920-8542},
  url        = {https://doi.org/10.1007/s11227-017-2141-4},
  doi        = {10.1007/s11227-017-2141-4},
  abstract   = {Cloud environments today are increasingly featuring hybrid nodes containing multicore CPU processors and a diverse mix of accelerators such as Graphics Processing Units (GPUs), Intel Xeon Phi co-processors, and Field-Programmable Gate Arrays (FPGAs) to facilitate easier migration to them of HPC workloads. While virtualization of accelerators in clouds is a leading research challenge, we address the programming challenges that assail execution of large instances of data-parallel applications using these accelerators in this paper. In a typical hybrid node in a cloud, the tight integration of accelerators with multicore CPUs via PCI-E communication links contains inherent limitations such as limited main memory of accelerators and limited bandwidth of the PCI-E communication links. These limitations poses formidable programming challenges to execution of large problem sizes on these accelerators. In this paper, we describe a library containing interfaces (HCLOOC) that addresses these challenges. It employs optimal software pipelines to overlap data transfers between host CPU and the accelerator and computations on the accelerator. It is designed using the fundamental building blocks, which are OpenCL command queues for FPGAs, Intel offload streams for Intel Xeon Phis, and CUDA streams and events that allow concurrent utilization of the copy and execution engines provided in NVidia GPUs. We elucidate the key features of our library using an out-of-core implementation of matrix multiplication of large dense matrices on a hybrid node, an Intel Haswell multicore CPU server hosting three accelerators that includes NVidia K40c GPU, Intel Xeon Phi 3120P, and a Xilinx FPGA. Based on experiments with the GPU, we show that our out-of-core implementation achieves 82\% of peak double-precision floating performance of the GPU and a speedup of 2.7 times over the NVidia's out-of-core matrix multiplication implementation (CUBLAS-XT). We also demonstrate that our implementation exhibits 0\% drop in performance when the problem size exceeds the main memory of the GPU. We observe this 0\% drop also for our implementation for Intel Xeon Phi and Xilinx FPGA.},
  journal    = {J. Supercomput.},
  month      = {02},
  pages      = {551-568},
  numpages   = {18},
  keywords   = {Research_Paper, Intel MKL, Heterogeneous clouds, CUDA, Intel Xeon Phi, GPU, CUBLAS, Matrix multiplication, Out-of-core}
}

@book{Intel_Xeon_Phi_Coprocessor_Architecture,
  author    = {Rahman, Rezaur. and SpringerLink (Online service)},
  title     = {Intel® Xeon Phi™ Coprocessor Architecture and Tools},
  publisher = {Apress :},
  year      = {2013.},
  address   = {Berkeley, CA :},
  keywords  = {Research_Paper},
  url       = {https://doi.org/10.1007/978-1-4302-5927-5}
}

@inproceedings{Power_Aware_GPU_Computing,
  author    = {Kasichayanula, Kiran and Terpstra, Dan and Luszczek, Piotr and Tomov, Stan and Moore, Shirley and Peterson, Gregory D.},
  booktitle = {2012 Symposium on Application Accelerators in High Performance Computing},
  title     = {Power Aware Computing on GPUs},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {64-73},
  abstract  = {Energy and power density concerns in modern processors have led to significant computer architecture research efforts in power-aware and temperature-aware computing. With power dissipation becoming an increasingly vexing problem, power analysis of Graphical Processing Unit (GPU) and its components has become crucial for hardware and software system design. Here, we describe our technique for a coordinated measurement approach that combines real total power measurement and per-component power estimation. To identify power consumption accurately, we introduce the Activity-based Model for GPUs (AMG), from which we identify activity factors and power for micro architectures on GPUs that will help in analyzing power tradeoffs of one component versus another using micro benchmarks. The key challenge addressed in this work is real-time power consumption, which can be accurately estimated using NVIDIA's Management Library (NVML). We validated our model using Kill-A-Watt power meter and the results are accurate within 10%. This work also analyses energy consumption of MAGMA (Matrix Algebra on GPU and Multicore Architectures) BLAS2, BLAS3 kernels, and Hessenberg kernels.},
  keywords  = {Research_Paper},
  doi       = {10.1109/SAAHPC.2012.26},
  issn      = {2166-515X},
  month     = {07}
}

@article{O(N)_Force_Calculation_Algorithm,
  author    = {Walter Dehnen},
  title     = {A Hierarchical (N) Force Calculation Algorithm},
  keywords  = {Research_Paper},
  doi       = {10.1006/jcph.2002.7026},
  url       = {https://doi.org/10.1006%2Fjcph.2002.7026},
  year      = {2002},
  month     = {06},
  publisher = {Elsevier (BV)},
  volume    = {179},
  number    = {1},
  pages     = {27--42},
  journal   = {Journal of Computational Physics}
}

@inproceedings{Irregular_Programs_GPUs_Study,
  author    = {Burtscher, Martin and Nasre, Rupesh and Pingali, Keshav},
  booktitle = {2012 IEEE International Symposium on Workload Characterization (IISWC)},
  title     = {A quantitative study of irregular programs on GPUs},
  year      = {2012},
  month     = {11},
  volume    = {},
  number    = {},
  pages     = {141-151},
  abstract  = {GPUs have been used to accelerate many regular applications and, more recently, irregular applications in which the control flow and memory access patterns are data-dependent and statically unpredictable. This paper defines two measures of irregularity called control-flow irregularity and memory-access irregularity, and investigates, using performance-counter measurements, how irregular GPU kernels differ from regular kernels with respect to these measures. For a suite of 13 benchmarks, we find that (i) irregularity at the warp level varies widely, (ii) control-flow irregularity and memory-access irregularity are largely independent of each other, and (iii) most kernels, including regular ones, exhibit some irregularity. A program's irregularity can change between different inputs, systems, and arithmetic precision but generally stays in a specific region of the irregularity space. Whereas some highly tuned implementations of irregular algorithms exhibit little irregularity, trading off extra irregularity for better locality or less work can improve overall performance.},
  keywords  = {Research_Paper},
  doi       = {10.1109/IISWC.2012.6402918},
  issn      = {}
}

# ===== EXTERNAL LINKS =====

@misc{NASA_Advanced_Supercomputing,
  author   = {NASA},
  title    = {NAS Parallel Benchmarks},
  keywords = {Useful_Link},
  url      = {https://www.nas.nasa.gov/software/npb.html},
  abstract = {The NAS Parallel Benchmarks (NPB) are a small set of programs designed to help evaluate the performance of parallel supercomputers. The benchmarks are derived from computational fluid dynamics (CFD) applications and consist of five kernels and three pseudo-applications in the original "pencil-and-paper" specification (NPB 1). The benchmark suite has been extended to include new benchmarks for unstructured adaptive meshes, parallel I/O, multi-zone applications, and computational grids. Problem sizes in NPB are predefined and indicated as different classes. Reference implementations of NPB are available in commonly-used programming models like MPI and OpenMP (NPB 2 and NPB 3).}
}

# Link to GitHub repository with software used for controlling the Yokogawa WT310E Power Meter
@misc{GitHub_intel/yoko-tool,
  author   = {Bityutskiy, Artem and Laakso, Antti and Correia, Helia},
  title    = {Yokotool},
  keywords = {Useful_Link},
  url      = {https://github.com/intel/yoko-tool},
  abstract = {Command-line tool for controlling Yokogawa power meters in Linux}
}

@misc{Energy_Consumption_of_ICT,
  author   = {The Parliamentary Office of Science and Technology},
  title    = {Energy Consumption of ICT},
  keywords = {Useful_Link},
  url      = {https://researchbriefings.files.parliament.uk/documents/POST-PN-0677/POST-PN-0677.pdf}
}

@misc{RAPL_Reporting,
  author   = {Intel},
  title    = {Running Average Power Limit Energy Reporting},
  keywords = {Useful_Link},
  url      = {https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/advisory-guidance/running-average-power-limit-energy-reporting.html}
}

@misc{NVML,
  author   = {NVIDIA},
  title    = {NVIDIA Management Library (NVML)},
  keywords = {Useful_Link},
  url      = {https://developer.nvidia.com/nvidia-management-library-nvml}
}

@misc{Yokogawa_Producent_Page,
  author   = {Yokogawa},
  title    = {WT300E Digital Power Analyzer},
  keywords = {Useful_Link},
  url      = {https://tmi.yokogawa.com/solutions/products/power-analyzers/digital-power-meter-wt300e/#Documents-Downloads}
}

@misc{Yokogawa_Meter_Series_Specifications,
  author   = {Yokogawa},
  title    = {WT300E Series Digital Power Meter},
  keywords = {Useful_Link},
  url      = {https://cdn.tmi.yokogawa.com/1/2562/files/BUWT300E-01EN.pdf}
}

@misc{NVML_Reference_Manual,
  author   = {NVIDIA},
  title    = {NVML API Reference Guide},
  keywords = {Useful_Link},
  url      = {https://docs.nvidia.com/deploy/nvml-api/index.html}
}

@misc{Intel_Xeon_Phi_Coprocessor_Developer,
  author   = {Intel},
  title    = {Intel® Xeon Phi™ Coprocessor DEVELOPER'S QUICK START GUIDE},
  keywords = {Useful_Link},
  url      = {https://www.intel.com/content/dam/develop/external/us/en/documents/intel-xeon-phi-coprocessor-quick-start-developers-guide.pdf}
}

@misc{NVIDIA_SMI,
  author   = {NVIDIA},
  title    = {System Management Interface SMI},
  keywords = {Useful_Link},
  url      = {https://developer.nvidia.com/nvidia-system-management-interface}
}

@misc{PAPI_CUDA,
  author   = {NVIDIA},
  title    = {PAPI CUDA Component},
  keywords = {Useful_Link},
  url      = {https://developer.nvidia.com/papi-cuda-component}
}

@misc{Pthreads,
  author   = {Michael Kerrisk},
  title    = {Pthreads - Linux manual page},
  keywords = {Useful_Link},
  url      = {https://man7.org/linux/man-pages/man7/pthreads.7.html}
}

@misc{OpenMP,
  author   = {OpenMP},
  title    = {OpenMP Reference Guides},
  keywords = {Useful_Link},
  url      = {https://www.openmp.org/resources/refguides/}
}

@misc{NVIDIA_Parallel_Thread_Execution,
  author   = {NVIDIA},
  title    = {Parallel Thread Execution ISA Version 8.2},
  keywords = {Useful_Link},
  url      = {https://docs.nvidia.com/cuda/parallel-thread-execution/index.html}
}

@misc{NVIDIA_NVCC,
  author   = {NVIDIA},
  title    = {NVIDIA CUDA Compiler Driver NVCC},
  keywords = {Useful_Link},
  url      = {https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/}
}
@misc{K20Power,
  author   = {Martin Burtscher},
  title    = {K20Power v1.1},
  keywords = {Useful_Link},
  url      = {https://userweb.cs.txstate.edu/~burtscher/research/K20power/}
}

@misc{LonestarGPU,
  author   = {ISS Group at the University of Texas},
  title    = {LonestarGPU},
  keywords = {Useful_Link},
  url      = {https://iss.oden.utexas.edu/?p=projects/galois/lonestargpu}
}

@misc{EnergyStar,
  author   = {ICR Polska},
  title    = {EnergyStar - ICR POLAND - testing and certification},
  keywords = {Useful_Link},
  url      = {https://icrpolska.com/en/energystar/}
}

@misc{SPEC,
  author   = {SPEC},
  title    = {SPECpower and Performance Committee},
  keywords = {Useful_Link},
  url      = {https://www.spec.org/power/}
}

@misc{IEC,
  author   = {International Electrotechnical Commission},
  title    = {IEC 62301:2011 | IEC Webstore | energy efficiency, smart city, standby power},
  keywords = {Useful_Link},
  url      = {https://webstore.iec.ch/publication/6789},
  abstract = {IEC 62301:2011 specifies methods of measurement of electrical power consumption in standby mode(s) and other low power modes (off mode and network mode), as applicable. It is applicable to electrical products with a rated input voltage or voltage range that lies wholly or partly in the range 100 V a.c. to 250 V a.c. for single phase products and 130 V a.c. to 480 V a.c. for other products. The objective of this standard is to provide a method of test to determine the power consumption of a range of products in relevant low power modes (see 3.4), generally where the product is not in active mode (i.e. not performing a primary function). This standard does not specify safety requirements. It does not specify minimum performance requirements nor does it set maximum limits on power or energy consumption. This second edition cancels and replaces the first edition published in 2005 and constitutes a technical revision. The main changes from the previous edition are as follows:
              - greater detail in set-up procedures and introduction of stability requirements for all measurement methods to ensure that results are as representative as possible;
              - refinement of measurement uncertainty requirements for power measuring instruments, especially for more difficult loads with high crest factor and/or low power factor;
              - updated guidance on product configuration, instrumentation and calculation of measurement uncertainty;
              - inclusion of definitions for low power modes as requested by TC59 and use of these new definitions and more rigorous terminology throughout the standard;
              - inclusion of specific test conditions where power consumption is affected by ambient illumination.}
}

@misc{iTeh,
  author   = {iTeh Standards},
  title    = {Electrical and electronic household and office equipment - Measurement of low power consumption},
  keywords = {Useful_Link},
  url      = {https://standards.iteh.ai/catalog/standards/clc/371d2d67-a439-4f20-96f0-02675496fd03/en-50564-2011}
}

@misc{Linux_taskset,
  author   = {Linux},
  title    = {"taskset" - Linux manual page},
  keywords = {Useful_Link},
  url      = {https://man7.org/linux/man-pages/man1/taskset.1.html}
}

@misc{Horovod_IDRIS,
  author   = {The Institute for Development and Resources in Intensive Scientific Computing},
  title    = {Horovod: Multi-GPU and multi-node data parallelism},
  keywords = {Useful_Link},
  url      = {http://www.idris.fr/eng/jean-zay/gpu/jean-zay-gpu-hvd-tf-multi-eng.html}
}

@misc{Google_Machine_Learning_Crash_Course,
  author   = {Google},
  title    = {Machine Learning Crash Course with TensorFlow APIs},
  keywords = {Useful_Links},
  url      = {https://developers.google.com/machine-learning/crash-course}
}

@misc{Kaggle,
  author   = {Kaggle},
  title    = {Kaggle: Your Machine Learning and Data Science Community},
  keywords = {Useful_Links},
  url      = {https://www.kaggle.com/}
}

@misc{Kaggle_Dataset,
  author   = {Gerry Piosenka},
  title    = {Birds 525 Species - Image Classification},
  year     = {2023},
  month    = {5},
  keywords = {Useful_Links},
  url      = {https://www.kaggle.com/datasets/gpiosenka/100-bird-species}
}

https://www.kaggle.com/datasets/gpiosenka/100-bird-species

@misc{Horovod,
  author   = {Uber Technologies},
  title    = {Horovod documentation},
  keywords = {Useful_Links},
  url      = {https://horovod.readthedocs.io/en/latest/index.html/}
}

@misc{WattsUp_Quick_Reference_Guide,
  author   = {Electronic Educational Devices},
  title    = {WattsUp? Operators Manual},
  keywords = {Useful_Links},
  url      = {https://arcb.csc.ncsu.edu/~mueller/cluster/arc/wattsup/metertools-1.0.0/docs/meters/wattsup/manual.pdf}
}
