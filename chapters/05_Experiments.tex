\chapter{Experiments}

\section{Proposed workflow and methodology}

In order to obtain the details of how system-level physical measurement
estimates energy consumption by a component (such as a CPU or a GPU)
during an application execution, several steps must be taken:

\begin{enumerate}
    \item Exclusive reservation of the entire computational node.
    \item Observation of the disk consumption and network usage before and
    during tests.
    \item Monitoring of the CPUs and GPUs utilization before and during tests.
    \item Running the benchmark kernels on an abstract processor only.
    Abstract processor comprises of the multicore CPU processor consisting
    of a certain number of physical cores and DRAM\@.
    \item Gathering of the power measurements.
    \item Verification of the accuracy and reliability of the software
    measurements tools, based on ground truth results.
\end{enumerate}

One of the notable mentions that could be done in order to reduce the amount
of uncertain power draw measurements done by the background components is
setting the fans to full speed. This solution have a potential of reducing
power draw fluctuations, especially during higher workloads, i\. e\. when
running benchmarks kernels utilizing maximum amount of GPUs or running Hybrid
configuration, where power draw of the entire nodes are very high. This could
not be implemented, however, as the administrator of the department's servers
stated, that interference in servers fans could be crucial for the nodes
stability.

\textbf{For Intel RAPL / NVIDIA Management Library:}
\begin{enumerate}
    \item Obtain base power of idle CPUs / GPUs.
    \item Obtain execution time of benchmark application.
    \item Obtain total energy consumption of the CPUs / GPUs during tests.
    \item Calculate dynamic energy consumption by subtracting base energy from
    total energy used during run.
\end{enumerate}

\textbf{For Yokotool:}
\begin{enumerate}
    \item Obtain base power of idle CPUs.
    \item Obtain execution time of benchmark application.
    \item Obtain total energy consumption of the CPUs during tests.
    \item Calculate dynamic energy consumption by subtracting base power from
    total energy used during run
\end{enumerate}

\newpage

In addition to the main experiments workflow, another methodology must be
adapted $-$ the data collection methodology. In order for the results to be
properly comparable, several point have to be met:
\begin{enumerate}
    \item Tests environment must be identical in every case, to eliminate
    discrepancy of the results.
    \item The results of the power draw reading must be properly compared for
    the device only measurements (Intel RAPL / NVML) and the measurements of
    the entire node (Yokotool).
    \item Experiments should be conducted on different nodes that utilizes
    different hardware, in order to state repeatability of tests.
    \item Experiments should be conducted, using different benchmark kernels
    or application, to remove the possibility of bias of the results, due to
    poor diversification of test cases.
    \item Test runs must be repeated many times.
\end{enumerate}

\section{Main tests}

\subsection{Overview on the scheduler script}

In order to automate the experiments, an entire scheduler script had to be
created. Its has two main tasks: first is to store the information about the
configurations and run the benchmarks according to the presets. The second
is to run the measurements softwares and save all the results in the ordered
manner.

Both of these goals are reached by using dictionaries with key-value pairs.
That created a~tree-like dependencies between the corresponding layers of
configurations. Finally, that solution works for both choosing the right
config and providing the path to save measurements.

\newpage

% \subsection{Main automation function $-$ scheduler()}
\subsection{Main automation function}

% NOTE TO SELF: Make sure this chart belongs to this section

The entire scheduler script consists of classes and functions that are
explicitly designated for their purposes in the code. Overview of them is
as follows:

\begin{itemize}
    \item \textbf{class `Config'} $-$ Contains all the informations about the servers,
    devices, implementations, benchmarks and configurations, handles the
    relations between them and provides correct pathes to the corresponding
    measurements directories.
    \item \textbf{class `Benchmark'} $-$ Defines functions responsible for
    executing CPUs and GPUs benchmarks, as well as the measurements
    softwares: Linux Perf, NVIDIA Management Library and Yokotool
    \item \textbf{class `Execution'} $-$ This class contains functions tasked with
    calling the benchmark kernels and checking their status, if they are still
    running, for the purpose of ending the measurements. Since the
    measurements softwares are highly dependent on benchmarks being run, they
    are executed directly from the \emph{main\@()} function and
    the \emph{Execution} class only has the functions tasked with the proper
    termination of them.
    \item \textbf{function `scheduler\@()'} $-$ This major function triggers
    secondary functions from \emph{Execution} class and watched their status.
    \item \textbf{function `main\@()'} $-$ Runs every configuration
    sequentially, based on the lists of presets. Additionally, repeats every
    ten times in order to achieve repeatability of the experiments.
\end{itemize}

In order to visualize the entire workflow of the scheduler, as well as the 
workings of the individual processes, two charts has been created:
\begin{itemize}
    \item \textbf{General Flowchart} $-$ This chart
    (\textbf{Fig.~\ref{fig:general_flowchart}}) describes the relations
    between the currently used configurations and the instructions executed
    based on those conditions.
    \item \textbf{Processes Flowchart} $-$ This chart
    (\textbf{Fig.~\ref{fig:processes_flowchart}}) shows the working priciples of the
    \emph{runner\@()} subfunction, which shows the benchmarks and measurements
    softwares are started on a~high level of abstraction.
\end{itemize}

\newpage

\begin{figure}[hbtp!]
    \centering
    \includegraphics{general_flowchart}
    \caption{General Flowchart}~\label{fig:general_flowchart}
\end{figure}

\begin{figure}[hbtp!]
    \centering
    \includegraphics{processes_flowchart.jpeg}
    \caption{Processes Flowchart}~\label{fig:processes_flowchart}
\end{figure}

\newpage

% \subsection{Threads pinning and kernels execution $-$ cpu\_benchmark()}
\subsection{Threads pinning and kernels execution}

This function is responsible of executing CPUs benchmarks, based on the
given configuration. It creates a separate processes by utilizing the
Python \emph{subprocess} module. 

\begin{lstlisting}[language=Python]
    cpu_benchmark = subprocess.Popen(
        [
            "taskset --cpu-list <T> <P> <B> > /dev/null 2>&1"
        ],
        shell=True
    )
    return cpu_benchmark.pid
\end{lstlisting}

Here is an explanation of every part of the command:

\begin{itemize}
    \item \textbf{cpu\_benchmark} $-$ A variable of type
    \emph{<class `subprocess.Popen'>} is created mainly in order to
    retrieve PID later on.
    \item \textbf{subprocess.Popen} $-$ The underlying process creation and
    management is handled by the Popen class. Its function is to execute
    a child program in a new process.
    \item \textbf{taskset} $-$ This command is used to set or retrieve the
    CPU affinity of a running process given its pid, or to launch a new
    command with a given CPU affinity.
    \item \textbf{--cpu-list} $-$ This option interprets mask as numerical
    list of processors instead of a bitmask. Numbers are separated by
    commas and may include ranges. For example: 0,5,8-11.
    \item Variables that dynamically changes based on the configurations:
    \begin{conditions}
        \textbf{T} & Logical processors indexes \\
        \textbf{P} & Specified absolute path to the correct measurements folder \\
        \textbf{B} & Currently used benchmark kernel \\
    \end{conditions}
    \item \textbf{\textgreater~/dev/null 2\textgreater\&1} $-$
    Redirecting \emph{stderr} containing error messages from the
    executed command or script to \emph{stdout} to the output of the
    command. Both are, in fact, redirected then to the so-called
    \emph{null device}. The result of that action is suppression of all
    messages printed by the benchmark kernels. It is useful when
    collecting logs from the terminal, that is running the entire
    scheduler script, without unnecessary messages.
    \item \textbf{shell=True} $-$ Invokes the program as `shell'
\end{itemize}

Finally, the function returns the PID of newly created process
as an integer value. It is done for the purpose of terminating the
benchmark in a Hybrid configuration.

\newpage

% \subsection{GPUs and threads management $-$ gpu\_benchmark\@()}
\subsection{GPUs and threads management}

This function consists of two parts: first part is responsible for executing
the Horovod-Python benchmark and the second part is responsible for running
the OMP-CUDA benchmark.

\begin{lstlisting}[language=Python]
    gpu_benchmark = subprocess.Popen(
        [
            "mpirun -np <N> --map-by socket -x NCCL_DEBUG=INFO \
            python3 <P>+"Xception.py > /dev/null 2>&1"
        ],
        shell=True
    )
    return gpu_benchmark.pid
\end{lstlisting}

Here is an explanation of every part of the command:

\begin{itemize}
    \item \textbf{gpu\_benchmark} $-$ A variable of type
    \emph{<class `subprocess.Popen'>} is created mainly in order to
    retrieve PID later on.
    \item \textbf{subprocess.Popen} $-$ The underlying process creation and
    management is handled by the Popen class. Its function is to execute
    a child program in a new process.
    \item \textbf{mpirun} $-$ This command is used to execute serial and
    parallel jobs. It will run X copies of specified program in the current
    run-time environment and scheduling (by default) in a~round-robin fashion
    by CPU slot.
    \item \textbf{-np} $-$ This option specifies, how many processes will be
    started.
    \item Variables that dynamically changes based on the configurations:
    \begin{conditions}
        \textbf{N} & Number of GPUs used in training \\
        \textbf{P} & Specified absolute path to the correct measurements folder \\
    \end{conditions}
    \item \textbf{--map-by socket} $-$ Map to the specified object, such as 
    slot, hwthread, core, socket, numa, board, node and more. In this
    particular case, benchmark application is mapped by `socket', allowing to
    utilize multiple GPUs for training in a ditributed manner.
    \item \textbf{-x} $-$ Export the specified environment variables to the
    remote nodes before executing the program.
    \item \textbf{NCCL\_DEBUG=INFO} $-$ This flag is used for debugging. In
    case of NCCL failure, you can set NCCL\_DEBUG=INFO to print an explicit
    warning message as well as basic NCCL initialization information.
    \item \textbf{python3} $-$ Specify the use of Python interpreter when
    executing the script of deep neural networks model training.
    \item \textbf{Xception.py} $-$ Name of the script file.
    \item \textbf{\textgreater~/dev/null 2\textgreater\&1} $-$
    As mentioned in the previous subsection, this command suppresses the
    output from the terminal, in order to avoid the unnecessary messages.
    \item \textbf{shell=True} $-$ Invokes the program as `shell'
\end{itemize}

Finally, the function returns the PID of newly created process
as an integer value. It is done for the purpose of terminating the
benchmark in a Hybrid configuration.

\newpage

The second part of the function that runs the OMP-CUDA benchmarks is as
follows:

\begin{lstlisting}[language=Python]
    list_of_gpu_benchmarks = []
    value = Config.taskset_gpu[configuration_gpu]
    for i in range(0, len(value), 1):
        gpu_benchmark = subprocess.Popen(
            [
                "taskset --cpu-list <I> <P+I> <B> > /dev/null 2>&1"
            ],
            shell=True,
        )
        list_of_gpu_benchmarks.append(gpu_benchmark.pid)
    return list_of_gpu_benchmarks
\end{lstlisting}

Here is an explanation of every part of the command:

\begin{itemize}
    \item \textbf{gpu\_benchmark} $-$ A variable of type
    \emph{<class `subprocess.Popen'>} is created mainly in order to
    retrieve PID later on.
    \item \textbf{subprocess.Popen} $-$ The underlying process creation and
    management is handled by the Popen class. Its function is to execute
    a child program in a new process.
    \item \textbf{taskset} $-$ This command is used to set or retrieve the
    CPU affinity of a running process given its pid, or to launch a new
    command with a given CPU affinity.
    \item \textbf{--cpu-list} $-$ This option interprets mask as numerical
    list of processors instead of a bitmask. Numbers are separated by
    commas and may include ranges.
    \item Variables that dynamically changes based on the configurations:
    \begin{conditions}
        \textbf{I} & \parbox[t]{12cm}{Allocation of individual logical
        processors to the GPUs, based on the index number from special
        dictionary} \\
        \textbf{P+I} & \parbox[t]{12cm}{Specified absolute path to
        the correct measurements folder, modified by the number of total GPUs
        used} \\
        \textbf{B} & Currently used benchmark kernel \\
    \end{conditions}
    \item \textbf{\textgreater~/dev/null 2\textgreater\&1} $-$
    As mentioned in the previous subsection, this command suppresses the
    output from the terminal, in order to avoid the unnecessary messages.
    \item \textbf{shell=True} $-$ Invokes the program as `shell'
    \item \textbf{list\_of\_gpu\_benchmarks.append\@(gpu\_benchmark.pid)} $-$
    Filling the list with PIDs of benchmarks for later termination.
\end{itemize}

Finally, the function returns the PID of newly created process
as an integer value. In this particular case, if there are more than one GPUs
used in tests, the PIDs of all spawned processes are parsed as a list to the
function that is responsible in orderly terminating all the kernels.

\newpage

% \subsection{Measurements with Yokotool software $-$ yoko\@()}
\subsection{Measurements with Yokotool software}

This function utilizes the high-level Python wrapper for Yokogawa WT310E Power
Meter, the \emph{Yokotool}.

Syntax is similar to the previous examples:

\begin{lstlisting}[language=Python]
    yokotool = subprocess.Popen(
        [
            "yokotool read T,P -o <P+N> > /dev/null 2>&1 &"
        ],
        shell=True
    )
    return yokotool.pid
\end{lstlisting}

Here is an explanation of every part of the command:

\begin{itemize}
    \item \textbf{yokotool} $-$ Yokotool's command line interface is based
    on commands and sub-commands, similar to git and many other tools. This
    invokes the Yokotool wrapper for use.
    \item \textbf{read} $-$ Read measurements data.
    \item \textbf{T,P} $-$ Specifies, what data we want to read. In this
    particular case it is time \textbf{T} measured from the start of the epoch
    (on `UNIX time' is starts at 00:00:00 UTC on 1 January 1970) and power
    \textbf{P}, measured in Watts [W]. Output is separated with comma for
    easy manipulation of data after tests.
    \item \textbf{-o} $-$ This flags redirects the output from the
    measurements to a file for later analysis.
    \item Variables that dynamically changes based on the configurations:
    \begin{conditions}
        \textbf{P+N} & \parbox[t]{12cm}{Specified absolute path to
        the correct measurements folder, modified by the current number of
        iterations} \\
    \end{conditions}
    \item \textbf{\textgreater~/dev/null 2\textgreater\&1} $-$
    As mentioned in the previous subsection, this command suppresses the
    output from the terminal, in order to avoid the unnecessary messages.
    In this case, and additional \& is placed at the end, which means that
    the entire command is put as a~background process.
    \item \textbf{shell=True} $-$ Invokes the program as `shell'
\end{itemize}

Finally, the function returns the PID of newly created process
as an integer value. It is done for the purpose of terminating the
measurements by the designated function.

\newpage

% \subsection{Measurements with Linux Perf software $-$ perf\@()}
\subsection{Measurements with Linux Perf software}

Linux Perf is a lightweight profiling tool with performance counters.
It utilizes Intel RAPL for measurements of pre-defined events on CPUs.
To obtain more informations about what can be measured by Linux Perf,
one can use \emph{perf list} command in the terminal.

\begin{lstlisting}[language=Python]
    list_of_perf_pids = []
    pin_to_cpus = ("0", "10")
    idx_names = {"0": "0", "10": "1"}
    for i in pin_to_cpus:
        perf = subprocess.Popen(
            [
                "perf stat --event=power/energy-pkg/ \
                --cpu=<C> --delay 100 --interval-print 100 \
                --summary --field-separator , \
                --output <P+N> > /dev/null 2>&1 &"
            ],
            shell = True,
        )
        list_of_perf_pids.append(perf.pid)
    return list_of_perf_pids
\end{lstlisting}

Here is an explanation of every part of the command:

\begin{itemize}
    \item \textbf{perf} $-$ Invocation of measurements tool.
    \item \textbf{stat} $-$ Run a command and gather performance counter
    statistics.
    \item \textbf{--event=power/energy-pkg/} $-$ Event selector, in this case,
    the measured physical quantity is the energy usage of selected CPUs,
    during the benchmark kernel execution.
    \item \textbf{--cpu=<C>} $-$ Targeting of a specific CPU\. Correct integer
    value is based on the output of \emph{lscpu} command and the information
    about \emph{NUMA node\@(s)}.
    \item \textbf{--delay 100} $-$ A small delay of 100 [ms] is introduced in
    order to offset the slight delay of measurements of Yokogawa power meter.
    This solution has been introduced as a result of an observation during the
    preliminary tests. 
    \item \textbf{--interval-print 100} $-$ Measurements are performed with
    the same time interval of 100 [ms] set as two others measurements methods.
    \item \textbf{--summary} $-$ At the end of the measurements, an additional
    information about total energy used during tests, as well as the total
    measurements time is given. Mainly used during preliminary tests as an
    additional insight on gathered results.
    \item \textbf{--field-separator \,} $-$ Sets the output delimiter for
    easier access in softwares like LibreOffice $-$ every printed value is
    separated by commas `\,'
    \item \textbf{--output} $-$ Saves measurements to file. In this case,
    file names matches the consecutive runs.
    \item Variables that dynamically changes based on the configurations:
    \begin{conditions}
        \textbf{P+N} & \parbox[t]{12cm}{Specified absolute path to
        the correct measurements folder, modified by the current number of
        iterations} \\
    \end{conditions}
    \item \textbf{\textgreater~/dev/null 2\textgreater\&1} $-$
    Silences the output, redirecting it to null device and puts the process
    in the background.
    \item \textbf{shell=True} $-$ Invokes the program as `shell'
\end{itemize}

Finally, the function returns the PID of newly created process
as an integer value. It is done for the purpose of terminating the
measurements by the designated function.

% \subsection{Measurements with NVML handling function $-$ nvml\@()}
\subsection{Measurements with NVML handling function}

The method of gathering the measurements of power draw of GPUs, using NVIDIA
Management Library is a little different than in previously shown
implementations. No new processes are spawned, due to the fact, that the
measurements are handled by a special function already implemented in the
scheduler script. This function has two core parts: the first one is
responsible of precise executing the measurements every 100 [ms] $-$ it works
as an built-in scheduler, and the second part handled the invocation of
NVML-specific functions and saving the results to the file.

A short code-snippet below illustrates the practical usage of NVML related
function

\begin{lstlisting}[language=Python]
    import py3nvml
    nvmlInit()
    handle_Idx0 = nvmlDeviceGetHandleByIndex(0)
    # based on number of GPUs used, there are more
    # variables of that kind
    measure_Idx0 = nvmlDeviceGetPowerUsage(handle_Idx0) / 1000.0
    # rest of code contains the sub-scheduler routine
    # and saves the output to file.
\end{lstlisting}

\begin{itemize}
    \item \textbf{import py3nvml} $-$ Import of a module, that function as
    a high level Python wrapper for NVML\@.
    \item \textbf{nvmlInit\@()} $-$ NVIDIA Management Library initialization.
    It is mandatory to run before calling any other methods.
    \item \textbf{handle\_Idx0 = nvmlDeviceGetHandleByIndex\@(0)} $-$ create
    variable \emph{handle\_Idx0} and assign the return value of data type
    \emph{<class `py3nvml.py3nvml.LP\_struct\_c\_nvmlDevice\_t'>}, which is,
    in fact, a pointer to a memory register containing data about specific
    GPU\@.
    \item \textbf{measure\_Idx0 = nvmlDeviceGetPowerUsage\@(handle\_Idx0) / 1000.0}
    $-$ This function return a value of power draw of a specified GPU, in
    Watts [W]. 
\end{itemize}

In terms of proper ending the measurements process, this task is handled quite
differently than in previous implementations. The \emph{nvml\@()} function is
started as a parallel thread, using \emph{multiprocessing.Process\@()} module.
The thread is then started as a daemon, collecting measurements and saving
them in the background. After the end of benchmark kernel, the function is
ended by using innate multiprocessing function $-$ \emph{.terminate\@()}.

\newpage

\subsection{Termination of benchmarks in Hybrid configuration}

In order to make sure that the Hybrid benchmarks starts and ends at the same
time, an additional subroutine has been implemented. At first, the condition
is checked if used \emph{devices} in tests are currently set to \emph{Hybrid}.
Then, a special `while' loop is started, in which a special functions checks,
if CPUs and GPUs benchmarks are still running. They work by periodically
checking, if PID associated with respective kernels still exists. If the 
benchmark on one of the devices has ended, then the second one should be
terminated as well. The flag is passed to a function that, based on PID or
PIDs of the second benchmark, starts the cleaning process. Major facilitation
comes from the utilities of \emph{psutil} module. This solution allows to
easily find all the child processes spawned by the parent benchmark process,
terminate them iteratively in a ordered manner, and finally, terminate the
parent process. After that, the measurements threads are terminated via
another, designated for that function and the current run is ended.

\subsection{Cleanup of measurements daemons}

In terms of the termination of measurements daemons, a single function
handles Linux Perf, NVIDIA Management Library and Yokotool. It utilizes two
popular Python modules: \emph{os} and \emph{signal}. The first module provides
a portable way of using operating system dependent
functionality~\cite{Python_os_module}, while the second allows defining custom
handlers to be executed when a signal is received~\cite{Python_signal_module}.
In case of Perf and Yokotool, at first, the PIDs of those processes are being
received, either by using \emph{pidof} or \emph{pigrep} of those daemons.
Then, they are terminated using following, quite self-explanatory command:

\begin{lstlisting}[language=Python]
    os.kill(<PID>, signal.SIGTERM)
\end{lstlisting}

Since measurements of GPUs power draw that uses NVML are incorporated directly
into the scheduler script as an function, the entire thread is being handled
and terminated by using the function, that comes from \emph{multiprocessing}
module $-$ \emph{.terminate\@()}.

\section{Analysis of the results and discussion}

This section contains a summary of tables with measurements results as well as
the analysis of the dependencies between individual results.

The first set of tables contains the information about the average power draw
and average energy used for every configuration chosen during the preliminary
tests. The tables themselves are self-explanatory, the captions associated
with them gives hints about used server, device, implementation, benchmark
and finally, the data displayed. 

Two things should be noted, however. First, the physical quantities with label
(Yokogawa) means that the measurements has been done using Yokogawa WT310E
Power Meter and are related to the entire node, while those with labels
(CPU\@: <index>) and (GPU\@: <index>) are related to measurements done using
Linux Perf and NVML respectively. Second, the configurations execution
times vary $-$ majority of the tests have shorter execution times the more
resources of used devices are dedicated. Such configurations are\@:
OMP-CPP benchmarks for \emph{sanna.kask}, MPI-Fortran and Horovod-Python
for \emph{vinnana.kask} and their hybrid variants. This is due to the fact,
those implementations utilize parallel programmings paradigms, therefore the
execution speed-up can be observed. In case of the OMP-CUDA benchmarks,
situation is different $-$ those benchmarks were prepared for single GPU tests
only. Since the main purpose of the benchmarks is to simply strain and fully
utilize the devices resources, rather than compute meaningful research
problems, the OMP-CUDA benchmarks on multi-GPUs are simply many instances of
the same benchmark, running on different GPUs. Since those kernels are
independent from each other, the average execution times between configurations
are rougly the same. This also explains the fact, that the Hybrid benchmarks
that were run on \emph{sanna.kask} has sometimes shorter execution times the
more resources are dedicated and sometimes and sometimes they are similar.
The Hybrid benchmarks works in a way, that the listener function from the
scheduler script checks, whether the benchmarks are running or not. If the
CPUs benchmarks ends, the termination signal is send to GPUs benchmarks,
resulting with varying execution times. If the GPUs benchmarks in given
configuration ends faster, the CPUs benchmarks are terminated in return,
resulting in similar execution times.



\newpage

\input{tables/05_OMP-CPP_1CPU_btC_power.tex}

\input{tables/05_OMP-CPP_1CPU_btC_energy.tex}

\input{tables/05_OMP-CPP_2CPUs_btC_power.tex}

\input{tables/05_OMP-CPP_2CPUs_btC_energy.tex}



\input{tables/05_OMP-CPP_1CPU_isD_power.tex}

\input{tables/05_OMP-CPP_1CPU_isD_energy.tex}

\input{tables/05_OMP-CPP_2CPUs_isD_power.tex}

\input{tables/05_OMP-CPP_2CPUs_isD_energy.tex}



\input{tables/05_OMP-CPP_1CPU_luC_power.tex}

\input{tables/05_OMP-CPP_1CPU_luC_energy.tex}

\input{tables/05_OMP-CPP_2CPUs_luC_power.tex}

\input{tables/05_OMP-CPP_2CPUs_luC_energy.tex}



\input{tables/05_OMP-CUDA_epD_power.tex}

\input{tables/05_OMP-CUDA_epD_energy.tex}

\input{tables/05_OMP-CUDA_luD_power.tex}

\input{tables/05_OMP-CUDA_luD_energy.tex}

\input{tables/05_OMP-CUDA_spD_power.tex}

\input{tables/05_OMP-CUDA_spD_energy.tex}



\input{tables/05_Hybrid_btC+luD_power.tex}

\input{tables/05_Hybrid_btC+luD_energy.tex}

\input{tables/05_Hybrid_isD+spD_power.tex}

\input{tables/05_Hybrid_isD+spD_energy.tex}

\input{tables/05_Hybrid_luC+epD_power.tex}

\input{tables/05_Hybrid_luC+epD_energy.tex}

\newpage

\input{tables/05_MPI-Fortran_epDx_power.tex}

\input{tables/05_MPI-Fortran_epDx_energy.tex}

\input{tables/05_MPI-Fortran_isDx_power.tex}

\input{tables/05_MPI-Fortran_isDx_energy.tex}

\input{tables/05_MPI-Fortran_luCx_power.tex}

\input{tables/05_MPI-Fortran_luCx_energy.tex}



\input{tables/05_XCeption_power.tex}

\input{tables/05_XCeption_energy.tex}



\input{tables/05_MPI_epD_power.tex}

\input{tables/05_MPI_epD_energy.tex}

\input{tables/05_MPI_isD_power.tex}

\input{tables/05_MPI_isD_energy.tex}

\input{tables/05_MPI_luC_power.tex}

\input{tables/05_MPI_luC_energy.tex}


\newpage
\FloatBarrier

In order to obtain information about the changes of power draw measurements
behaviour during the execution of benchmarks, a base values of idle nodes
power draw should be gathered as a reference for further investigation.
Idle nodes power draw has been measured as follows: each server has been
reserved for the duration of tests, with no other processes running in the
background. Then, using the Yokogawa WT310E Power Meter, the power draw of
entire servers has been measured independently for the period of one hour.
Additionally, the power draw of each CPU and each GPU has been measured
as well, using Linux Perf and NVML respectively. Finally, the base value
for further analysis has been calculated by substracting the sum of idle
CPUs and GPUs power draw from the idle node power draw. These values are
presented in the tables below as a reference:

\input{tables/idle_sanna.tex}

\input{tables/idle_vinnana.tex}

Last set of tables presented on the next pages serve as a summary of
differences in power draw between the values measured during the execution
of benchmark kernels and the base value of idle node power draw.
Explanation of data presented in summary tables is shown below:

\begin{itemize}
    \item \textbf{Difference} $-$ substraction of active node and the sum of
    active CPUs and GPUs power draw [W]
    \item \textbf{Offset} $-$ difference between the substraction of active
    node and the sum of active CPUs and GPUs power draw [W] and substraction
    of idle node and the sum of idle CPUs and GPUs power draw [W]
    \item \textbf{Increase} $-$ percentage increase in node power draw during
    tests, compared to idle node power draw [\%]
\end{itemize}

\newpage
\FloatBarrier

\input{tables/summary/omp-cpp1-btC.tex}

\input{tables/summary/omp-cpp1-isD.tex}

\input{tables/summary/omp-cpp1-luC.tex}

\newpage

\input{tables/summary/omp-cpp2-btC.tex}

\input{tables/summary/omp-cpp2-isD.tex}

\input{tables/summary/omp-cpp2-luC.tex}

\newpage

\input{tables/summary/omp-cuda-epD.tex}

\input{tables/summary/omp-cuda-luD.tex}

\input{tables/summary/omp-cuda-spD.tex}

\newpage

\input{tables/summary/omp-hybrid-btC-luD.tex}

\input{tables/summary/omp-hybrid-isD-spD.tex}

\input{tables/summary/omp-hybrid-luC-epD.tex}

\newpage

\input{tables/summary/mpi-cpu-epD.tex}

\input{tables/summary/mpi-cpu-isD.tex}

\input{tables/summary/mpi-cpu-luC.tex}

\newpage

\input{tables/summary/mpi-gpu-xception.tex}

\input{tables/summary/mpi-hybrid-epD.tex}

\input{tables/summary/mpi-hybrid-isD.tex}

\input{tables/summary/mpi-hybrid-luC.tex}

In this section, the conclusion are drawn based on the interpretation of
summary tables of differences in power draw during various tests
configurations.

\begin{itemize}
    \item \textbf{CPUs benchmarks on \emph{sanna.kask}} $-$ based on the
    observation of power draw offsets, the conclusion is that the power draw
    of entire node grows almost linearly, the more logical processors are used
    for running the benchmarks. Biggest change occurs when the tests are
    started at all, in the \emph{1 CPU 1 Thread} and \emph{2 CPUs 2 Threads}
    configurations, and gets smaller, the more resources are dedicated to
    tests.
    \item \textbf{GPUs benchmarks on \emph{sanna.kask}} $-$ in this set
    of tests one can observe an interesting phenomenon. At first, situation
    seems similar to CPUs benchmarks in terms of near linear offset changes,
    when 1 or 2 GPUs are used for the tests. When benchmarks are executed on
    4 or 8 GPUs, however, we can observe a significant surge in power draw of
    the entire node. This could be related to the increased activity of other
    components of the server, such as increased power draw of motherboard,
    increased fans speed or greater RAM usage.
    \item \textbf{Hybrid benchmarks on \emph{sanna.kask}} $-$ this set gives
    similar results to the GPUs-only set. The increase in power draw of the
    whole server, when executing benchmarks
    on 4 GPUs + 16 Threads and 8 GPUs + 32 Threads is even more visible.
    Again, the conclusions are similar to the analysis of the GPUs benchmarks.
    \item \textbf{CPUs benchmarks on \emph{vinnana.kask}} $-$ analysis of the
    results from this configurations confirm the observation of CPUs benchmarks
    on \emph{sanna.kask}. This time, the offset of node power draw seems to be
    near linear as well, again with slightly bigger difference at lower
    processes count and decreasing the more processes are being spawned.
    \item \textbf{GPUs benchmarks on \emph{vinnana.kask}} $-$ interesting
    difference between these GPU benchmarks and the benchmarks executed on
    \emph{sanna.kask} is fact, that the results of the tests performed
    on \emph{vinnana.kask} are more similar to the CPU benchmarks. The power
    draw surge is not observed anymore, when executing the training on the
    configuration with 4~GPUs, leading to a more linear changes in offset and
    percentage increase of node power draw compared to idle values.
    \item \textbf{Hybrid benchmarks on \emph{vinnana.kask}} $-$ finally, the
    last set of configurations. While the values of offsets and percentage
    increases are higher than those in GPUs benchmarks, overall the change
    is without major changes between configurations, unlike the tests
    performed on \emph{vinnana.kask}.
\end{itemize}