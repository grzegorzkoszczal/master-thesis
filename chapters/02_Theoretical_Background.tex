\chapter{Theoretical background}

\section{Measurement software}

\subsection{Intel RAPL}

Intel RAPL (Running Average Power Limit)
~\cite{Power_Management_on_Intel_Microprocessor} is an interface
~\cite{RAPL_Power_Estimation_and_Capping}, which allows
software to set a power limit that hardware ensures and any
power control system takes it as an input and tunes behavior
to ensure that this operating limit is respected.
That way, it is possible to set and monitor power limits both
on processor and DRAM, and by controlling the maximum average
power, it matches the expected power and cooling budget. RAPL
exposes its energy counters through model-specific registers
(MSRs) It updates these counters once in every 1 ms. The energy
is calculated as a multiple of model specific energy units.
For Sandy Bridge, the energy unit is 15.3 µJ, whereas it is 61
µJ for Haswell and Skylake.

Moreover, the Intel RAPL divides the platform into four domains,
which consists of:

\begin{itemize}
    \item PP0 (Core Devices) $-$ Includes the energy consumption
    by all the CPU cores in the socket(s).
    \item PP1 (Uncore Devices) $-$ Includes the power consumption
    of integrated graphics processing unit (unavailable on the
    server platforms)
    \item DRAM $-$ The energy consumption of the main memory.
    \item Package $-$ The energy consumption of the entire socket
    including core and uncore.
\end{itemize}

\subsection{NVIDIA NVML}

NVIDIA Management Library (NVML)~\cite{NVML} $-$ A C-based API
for monitoring and managing various states of the NVIDIA GPU
devices. It provides direct access to the queries and commands
exposed via nvidia-smi~\cite{NVIDIA_SMI}.
The runtime version of NVML ships
with the NVIDIA display driver, and the SDK provides the
appropriate header, stub libraries and sample applications.
Each new version of NVML is backwards compatible and is intended
to be a platform for building third party applications.

There are various techniques of computing the energy consumption
using the NVIDIA Management Library, which query the onboard
sensors and read the power usage of the device. Such techniques
are either from the native NVML API, like Sampling Monitoring
Approach (SMA) or Multi-Threaded Synchronized Monitoring (MTSM)
or from CUDA component and it's called Performance Application
Programming Interface (PAPI)~\cite{PAPI_CUDA}.

Sampling Monitoring Approach (SMA) $-$ The C-based API provided
by NVML that can query the power usage of the device and provide
an instantaneous power measurement. Therefore, it can be
programmed to keep reading the hardware sensor with a certain
frequency. The \emph{nvmlDeviceGetPowerUsage()} function is
used to retrieve the power usage reading for the device, in
milliwatts. This function is called and executed by the CPU\@.
The highest frequency possible is 66.7 Hz, which means the
measurements are done every 15 ms.

Performance Application Programming Interface (PAPI) provides
an API to access the hardware performance counters found on
modern processors. The various performance metrics can be read
through simple programming interfaces from C or Fortran. It
could be used as a middleware in different profiling and tracing
tools. PAPI can work as a high-level wrapper for different
components. Previously it used only Intel RAPL's interface to
report the power usage and energy consumption for Intel CPUs,
but recent updates added the NVML component, which supports
both measuring and capping power usage on modern NVIDIA GPU
architectures. The major advantage of using PAPI is that the
measurements are by default synchronized with the kernel execution.
The NVML component implemented in PAPI uses the function,
\emph{getPowerUsage()} which query \emph{nvmlDeviceGetPowerUsage()}
function. According to the documentation, this function is called
only once when the command “papi end” is called. Thus, the power
returned using this method is an instantaneous power when the
kernel finishes execution.

Multi-Threaded Synchronized Monitoring (MTSM) $-$ This method
differs from SMA approach in the measurement period, a specific,
exact window of the kernel execution is identified which results
in recording of only the power reading of the kernel solely.
Monitoring part is performed by the host CPU in that way the
master thread calls and then monitors the kernel and other
threads records the power, therefore it requires the use of
parallel programming execution models, such as Pthreads
~\cite{Pthreads} or
OpenMP~\cite{OpenMP}. This approach at first initializes the volatile
variable (at master thread) that is used later in recording of
power readings. Then, the remaining threads execute the monitoring
function in parallel and start measuring the time and power draw
as the benchmark kernel starts doing the computation. After its
work is done, the timing is ended and the stored measurements
are synchronized, giving the precise logs of power consumption
during the test period.

\section{Measurement hardware}

\subsection{Yokogawa WT310E}

In order to perform comparison and to check the reliability
of software power measurement methods, such as mentioned
above Intel RAPL or NVIDIA NVML, it is mandatory to use
a certified tool that would serve as the ground truth in
such tests. Such a tool is Yokogawa WT310E $-$ an external
power meter that will serve this purpose in tests in this paper.
It is a digital power analyzer that provides extremely low
current measurement capability down to 50 micro-Amps, and
a maximum of up to 26 Amps RMS\@. This device follows standards
and certificates such as Energy Star®~\cite{EnergyStar},
SPECpower~\cite{SPEC} and IEC62301~\cite{IEC} / EN50564~\cite{iTeh}
testing. This model comes from a WT300E's family of appliances
that offer a wide range of functions and enhanced specifications,
allowing handling of all the measurement applications from low
frequency to high frequency inverters using a single power meter.
The WT300E series with the fast display update rate of 100ms,
offer's engineers a short tact time in their testing procedures.
The basic accuracy for all input ranges is 0.1\% rdg + 0.05\%
rng (50/60Hz) and DC 0.1\% rdg + 0.2\% rng.

To use the Yokogawa WT310E power meter, a special software has
been written for easy use $-$ the Yokotool~\cite{GitHub_intel/yoko-tool}.
Yokotool is a command-line tool for controlling Yokogawa power
meters in Linux. The tool is written in Python and works with both
Python 2.7 and Python 3. The tool comes with the
`yokolibs.PowerMeter' module which can be used from Python scripts.

(Work-In-Progress $-$ Here I can cover a bit more of the use
of Yokotool in this project)

(Work-In-Progress: add honorable mentions, such as WattsUp
and Kill-A-Watt~\cite{Power_Aware_GPU_Computing} and their
use in previous works)

% Here I describe the benchmark applications
\section{Benchmark applications}

(Work-In-Progress)

[TO DO\@:

1. Cite 2 works from NPB-CPP and NPB-CUDA github repos

2. Write more about my own multi-gpu benchmark, maybe cite myself]

For the purpose of the experiments, benchmark applications should
fully utilize the resources of the tested CPUs and GPUs, as well as
be able to run on various configurations parameters. Such parameters
are: being able to run in parallel on various numbers of logical
processors, being able to run on one or more GPUs and being able to
execute on various input parameters class sizes.

There are four benchmark sets, that satisfies mentioned goals:
\begin{itemize}
    \item NAS Parallel Benchmarks (C++ with OMP)
    \item NAS Parallel Benchmarks (Fortran with MPI)
    \item NAS Parallel Benchmarks (CUDA)
    \item Custom deep learning model, based on XCeptionnet with MPI communication
\end{itemize}

In a general sense, the NAS Parallel Benchmarks are a set of programs designed
and created in order to help evaluate the performance of parallel
supercomputers. They are based on computational fluid dynamics applications and
originally consisted of five kernels and three pseudo-applications. Later on,
the benchmark suite has been extended with more kernels, such as adaptive meshes,
parallel I/O, multi-zone applications, and computational grids. Every application
comes with predefined and indicated problem size, labeled as class size.
Moreover, the benchmark kernels are available in commonly-used programming
models like MPI and OpenMP, which allows for easy configuration of use with
various number of CPU threads (NPB-OMP and NPB-MPI)~\cite{NPB-CPP}, as well
as execution on two or more computational nodes (NPB-MPI only). For the
computations on GPUs, different set has been created, which excels in tests
on a single devices (NPB-CUDA)~\cite{NPB-CUDA_1}~\cite{NPB-CUDA_2}.

The original set consists of eight benchmarks, which are tested later in the
experiments conducted for the purpose of this work.

Five kernels:
\begin{itemize}
    \item IS $-$ Integer Sort (random memory access)
    \item EP $-$ Embarassingly Parallel
    \item CG $-$ Conjugate Gradient (irregular memory access and communication)
    \item MG $-$ Multi-Grid on a sequence of meshes
    \item FT $-$ Discrete 3D fast Fourier Transform (all-to-all communication)
\end{itemize}

Three pseudo applications:
\begin{itemize}
    \item BT $-$ Block Tri-diagonal solver
    \item SP $-$ Scalar Penta-diagonal solver
    \item LU $-$ Lower-Upper Gauss-Seidel solver
\end{itemize}

The three pseudo applications mentioned earlier also comes with the multi-zone
versions, designed to exploit multiple levels of parallelism. Moreover, NPB
suite also offers benchmarks for unstructured computation, parallel I/O and
data movement. For the purpose of this work only the original single-zone
kernels and applications were used, therefore these benchmarks suites are
mentioned only.

In addition to solving different computational problems, each kernel or
application operates on various sizes of input data, determined during
compilation, known as classes. Those classes helps choosing the right benchmark
in term of execution time, which helps in measurements. Too short benchmarks may
cause measurement error, due to low sampling rate of measurement instruments and
too long benchmarks are unnecessary, because they prolong the overall experiments.

Benchmark classes:
\begin{itemize}
    \item Class S $-$ Very small, used for quick test purpose. Nowadays obsolete.
    \item Class W $-$ The so-called `90's workstation' size, nowadays consisted small.
    \item Classes A, B, C $-$ Standard test problems (~4 times size increase from
    each of the previous classes)
    \item Classes D, E, F $-$ Large test problems (~16 times size increase from
    each of the previous classes)
\end{itemize}


\subsection{NPB for CPU, C++ with OMP}

(Work-In-Progress)

Explain those benchmarks, how do they works, what problems do they
tackle, what are class sizes, how do they differ, why is it 
useful and so on.

Write that it works on one a two CPUs and on different number of
threads $-$ and prove it by screenshots or something.

DON'T EXPLAIN WHICH ONE YOU CHOSE, ADD THAT LATER IN ANOTHER CHAPTER
FOR EXAMPLE, `TEST METHODOLOGY' CHAPTER

\subsection{NPB for CPU, Fortran with MPI}

[PLACEHOLDER]

\subsection{NPB for GPU, CUDA}

(Work-In-Progress)

Most of content will be covered in previous subsection, so mention
how does it work for GPUs

Mention that it works on SINGLE GPU

\subsection{Custom Deep Neural Networks model}

Explain thoroughly Your own model and the fact it works 
on GPUs in distributed manner.