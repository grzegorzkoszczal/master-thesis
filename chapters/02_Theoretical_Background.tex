\chapter{Theoretical background}

\section{Measurement software}

\subsection{Intel RAPL}

Intel RAPL (Running Average Power Limit)
~\cite{Power_Management_on_Intel_Microprocessor} is an interface
~\cite{RAPL_Power_Estimation_and_Capping}, which allows
software to set a power limit that hardware ensures and any
power control system takes it as an input and tunes behavior
to ensure that this operating limit is respected.
That way, it is possible to set and monitor power limits both
on processor and DRAM, and by controlling the maximum average
power, it matches the expected power and cooling budget. RAPL
exposes its energy counters through model-specific registers
(MSRs) It updates these counters once in every 1 ms. The energy
is calculated as a multiple of model specific energy units.
For Sandy Bridge, the energy unit is 15.3 µJ, whereas it is 61
µJ for Haswell and Skylake.

Moreover, the Intel RAPL divides the platform into four domains,
which consists of:

\begin{itemize}
    \item PP0 (Core Devices) $-$ Includes the energy consumption
    by all the CPU cores in the socket\@(s).
    \item PP1 (Uncore Devices) $-$ Includes the power consumption
    of integrated graphics processing unit (unavailable on the
    server platforms)
    \item DRAM $-$ The energy consumption of the main memory.
    \item Package $-$ The energy consumption of the entire socket
    including core and uncore.
\end{itemize}

\subsection{NVIDIA NVML}

NVIDIA Management Library (NVML)~\cite{NVML} $-$ A C-based API
for monitoring and managing various states of the NVIDIA GPU
devices. It provides direct access to the queries and commands
exposed via nvidia-smi~\cite{NVIDIA_SMI}.
The runtime version of NVML ships
with the NVIDIA display driver, and the SDK provides the
appropriate header, stub libraries and sample applications.
Each new version of NVML is backwards compatible and is intended
to be a platform for building third party applications.

There are various techniques of computing the energy consumption
using the NVIDIA Management Library, which query the onboard
sensors and read the power usage of the device. Such techniques
are either from the native NVML API, like Sampling Monitoring
Approach (SMA) or Multi-Threaded Synchronized Monitoring (MTSM)
or from CUDA component and it's called Performance Application
Programming Interface (PAPI)~\cite{PAPI_CUDA}.

Sampling Monitoring Approach (SMA) $-$ The C-based API provided
by NVML that can query the power usage of the device and provide
an instantaneous power measurement. Therefore, it can be
programmed to keep reading the hardware sensor with a certain
frequency. The \emph{nvmlDeviceGetPowerUsage\@()} function is
used to retrieve the power usage reading for the device, in
milliwatts. This function is called and executed by the CPU\@.
The highest frequency possible is 66.7 Hz, which means the
measurements are done every 15 ms.

Performance Application Programming Interface (PAPI) provides
an API to access the hardware performance counters found on
modern processors. The various performance metrics can be read
through simple programming interfaces from C or Fortran. It
could be used as a middleware in different profiling and tracing
tools. PAPI can work as a high-level wrapper for different
components. Previously it used only Intel RAPL's interface to
report the power usage and energy consumption for Intel CPUs,
but recent updates added the NVML component, which supports
both measuring and capping power usage on modern NVIDIA GPU
architectures. The major advantage of using PAPI is that the
measurements are by default synchronized with the kernel execution.
The NVML component implemented in PAPI uses the function,
\emph{getPowerUsage\@()} which query \emph{nvmlDeviceGetPowerUsage\@()}
function. According to the documentation, this function is called
only once when the command “papi end” is called. Thus, the power
returned using this method is an instantaneous power when the
kernel finishes execution.

Multi-Threaded Synchronized Monitoring (MTSM) $-$ This method
differs from SMA approach in the measurement period, a specific,
exact window of the kernel execution is identified which results
in recording of only the power reading of the kernel solely.
Monitoring part is performed by the host CPU in that way the
master thread calls and then monitors the kernel and other
threads records the power, therefore it requires the use of
parallel programming execution models, such as Pthreads
~\cite{Pthreads} or
OpenMP~\cite{OpenMP}. This approach at first initializes the volatile
variable (at master thread) that is used later in recording of
power readings. Then, the remaining threads execute the monitoring
function in parallel and start measuring the time and power draw
as the benchmark kernel starts doing the computation. After its
work is done, the timing is ended and the stored measurements
are synchronized, giving the precise logs of power consumption
during the test period.

\section{Measurement hardware}

\subsection{Yokogawa WT310E}

In order to perform comparison and to check the reliability
of software power measurement methods, such as mentioned
above Intel RAPL or NVIDIA NVML, it is mandatory to use
a certified tool that would serve as the ground truth in
such tests. Such a tool is Yokogawa WT310E $-$ an external
power meter that will serve this purpose in tests in this paper.
It is a digital power analyzer that provides extremely low
current measurement capability down to 50 micro-Amps, and
a maximum of up to 26 Amps RMS\@. This device follows standards
and certificates such as Energy Star®~\cite{EnergyStar},
SPECpower~\cite{SPEC} and IEC62301~\cite{IEC} / EN50564~\cite{iTeh}
testing. This model comes from a WT300E's family of appliances
that offer a wide range of functions and enhanced specifications,
allowing handling of all the measurement applications from low
frequency to high frequency inverters using a single power meter.
The WT300E series with the fast display update rate of 100ms,
offer's engineers a short tact time in their testing procedures.
The basic accuracy for all input ranges is 0.1\% rdg + 0.05\%
rng (50/60Hz) and DC 0.1\% rdg + 0.2\% rng.

To use the Yokogawa WT310E power meter, a special software has
been written for easy use $-$ the Yokotool~\cite{GitHub_intel/yoko-tool}.
Yokotool is a command-line tool for controlling Yokogawa power
meters in Linux. The tool is written in Python and works with both
Python 2.7 and Python 3. The tool comes with the
`yokolibs.PowerMeter' module which can be used from Python scripts.

(Work-In-Progress $-$ Here I can cover a bit more of the use
of Yokotool in this project)

(Work-In-Progress: add honorable mentions, such as WattsUp
and Kill-A-Watt~\cite{Power_Aware_GPU_Computing} and their
use in previous works)

% Here I describe the benchmark applications
\section{Benchmark applications}

(Work-In-Progress)

[TO DO\@:

1. Cite 2 works from NPB-CPP and NPB-CUDA github repos TEST

2. Write more about my own multi-gpu benchmark, maybe cite myself]

For the purpose of the experiments, benchmark applications should
fully utilize the resources of the tested CPUs and GPUs, as well as
be able to run on various configurations parameters. Such parameters
are: being able to run in parallel on various numbers of logical
processors, being able to run on one or more GPUs and being able to
execute on various input parameters class sizes.

There are four benchmark sets, that satisfies mentioned goals:
\begin{itemize}
    \item NAS Parallel Benchmarks (C++ with OMP)
    \item NAS Parallel Benchmarks (Fortran with MPI)
    \item NAS Parallel Benchmarks (CUDA)
    \item Custom deep learning model, based on XCeptionnet with MPI communication
\end{itemize}

In a general sense, the NAS Parallel Benchmarks are a set of programs designed
and created in order to help evaluate the performance of parallel
supercomputers. They are based on computational fluid dynamics applications and
originally consisted of five kernels and three pseudo-applications. Later on,
the benchmark suite has been extended with more kernels, such as adaptive meshes,
parallel I/O, multi-zone applications, and computational grids. Every application
comes with predefined and indicated problem size, labeled as class size.
Moreover, the benchmark kernels are available in widely-used programming
models like MPI and OpenMP, which allows for easy configuration of use with
various number of CPU threads (NPB-OMP and NPB-MPI)~\cite{NPB-CPP}, as well
as execution on two or more computational nodes (NPB-MPI only). For the
computations on GPUs, different set has been created, which excels in tests
on a single devices (NPB-CUDA)~\cite{NPB-CUDA_1}~\cite{NPB-CUDA_2}. In order To
run GPUs benchmarks in multi-GPUs and multi-node environments, a custom deep
learning model has been created solely for the purpose of this task.

Below is listed the original set of eight NPB benchmarks, which are tested later in the
experiments conducted for the purpose of this work.

Five kernels:
\begin{itemize}
    \item \textbf{IS} $-$ Integer Sort (random memory access)
    \item \textbf{EP} $-$ Embarassingly Parallel
    \item \textbf{CG} $-$ Conjugate Gradient (irregular memory access and communication)
    \item \textbf{MG} $-$ Multi-Grid on a sequence of meshes
    \item \textbf{FT} $-$ Discrete 3D fast Fourier Transform (all-to-all communication)
\end{itemize}

Three pseudo applications:
\begin{itemize}
    \item \textbf{BT} $-$ Block Tri-diagonal solver
    \item \textbf{SP} $-$ Scalar Penta-diagonal solver
    \item \textbf{LU} $-$ Lower-Upper Gauss-Seidel solver
\end{itemize}

The three pseudo applications mentioned earlier also comes with the multi-zone
versions, designed to exploit multiple levels of parallelism. Moreover, NPB
suite also offers benchmarks for unstructured computation, parallel I/O and
data movement. For the purpose of this work only the original single-zone
kernels and applications were used, therefore these benchmarks suites are
mentioned only.

In addition to solving different computational problems, each kernel or
application operates on various sizes of input data, determined during
compilation, known as classes. Those classes helps choosing the right benchmark
in term of execution time, which helps in measurements. Too short benchmarks may
cause measurement error, due to low sampling rate of measurement instruments and
too long benchmarks are unnecessary, because they prolong the overall experiments.

Benchmark classes:
\begin{itemize}
    \item \textbf{Class S} $-$ Very small, used for quick test purpose. Nowadays obsolete.
    \item \textbf{Class W} $-$ The so-called `90's workstation' size, nowadays consisted small.
    \item \textbf{Classes A, B, C} $-$ Standard test problems ($\sim$4 times size increase from
    each of the previous classes)
    \item \textbf{Classes D, E, F} $-$ Large test problems ($\sim$16 times size increase from
    each of the previous classes)
\end{itemize}


\subsection{NPB for CPU, C++ with OMP}

NAS Parallel Benchmarks for CPU has been ported from Fortran to C++ $-$ a programming
language that has been developed for a long time as an Object-Oriented successor of
a very popular and successful C language. It is a strongly typed, highly portable
language with very high performance, compatibility and excellent yet difficult,
manual memory management, which makes it a grea choice for writing code, that
is meant to use in High Performance Computing, where fast execution times and low
memory usages are among the most important factors. C++ offers also many 
well-optimised libraries to choose from, based on our goals, as well as extensive
amount of documentations and books, explaining the principles of the language.

OpenMP (Open Multi-Processing) is an API (Application Programming Interface)
and set of directives for parallel programming in shared-memory multiprocessing
environments, primarily used for multi-threading. It's designed to simplify
the development of parallel applications by allowing developers to add parallelism
to their code through compiler directives and library functions. OpenMP is
particularly popular in scientific and high-performance computing applications
where performance optimization is crucial.

Since NPB-OMP comes with up to eight benchmarks and each benchmark can be compiled
with various class sizes, it creates a very diverse and flexible test cases to
choose from, depending of our needs. More informations about choosing the correct
configurations for our purposes are explained in Chapter 4: `Proposal of the
Solution'

Another major advantage of these benchmarks are the fact, that after the
compilation, one can be executed and pinned to specific logical processor using
Linux's \emph{taskset} command~\cite{Linux_taskset}, which is mainly used to
set or retrieve the CPU affinity of a running process. In practice, it allows
to set exact number of created benchmark processes and allows to manually pin
them to the desired threads, which gives us the absolute control over choosing
the benchmark configuration and gives us confidence, that they are correctly
executed.

\subsection{NPB for CPU, Fortran with MPI}

Another suite of benchmarks created for execution on CPUs $-$ an older set that
was written in the world's first high level programming language, the Fortran.
Fortran has been widely used in scientific, engineering and numerical computing
applications for several decades. Its main advantages such as high performance,
standarized libraries, support for complex numbers and parallel computing makes
it a valuable and reliable choice in numerical and scientific computing to this
day.

MPI (Message Passing Interface), is a standardized and portable message-passing
system designed for parallel and distributed computing. It provides a set of
routines and libraries for high-performance communication and coordination among
processes or tasks in a parallel or distributed computing environment. MPI is
particularly popular in high-performance computing (HPC) and scientific computing,
where large-scale parallelism is essential for solving complex problems. In
contrast to OMP, this implementation provides inter-node communication for
execution of code on several nodes in parallel manner.

Similar to NPB-OMP, this benchmarks suite also comes with many kernels and class
sizes to choose from, but it should be noted, that it does not offer complete
flexibility in therms of choosing the number of processes $-$ some kernels
require a certain number of processes to run which are specified in the code
documentation, and are caused by the way, the implementation was created.

\subsection{NPB for GPU, CUDA}

CUDA (Compute Unified Device Architecture), is a parallel computing platform
and application programming interface (API) developed by NVIDIA Corporation.
CUDA is specifically designed for accelerating general-purpose computational
tasks on NVIDIA GPUs (Graphics Processing Units). It allows developers to harness
the massive parallel processing power of GPUs to accelerate a~wide range of
applications, including scientific simulations, machine learning, image
processing, and more.

Key features and concepts of CUDA include, but are not limited, to:
\begin{itemize}
    \item Parallel Computing Model: CUDA enables developers to write parallel
    code that can be executed on GPUs.
    \item GPU Acceleration: CUDA provides a means to offload computationally
    intensive tasks from the CPU to the GPU\@.
    \item CUDA C/C++ Language Extensions: Developers can write CUDA code using
    C or C++ with CUDA-specific extensions. These extensions allow developers
    to define GPU kernels, which are functions that run in parallel on the GPU\@.
    \item CUDA Toolkit: The CUDA Toolkit includes the CUDA compiler, runtime
    libraries, and development tools. It enables developers to write, compile,
    and optimize CUDA applications.
\end{itemize}

As mentioned in the previously shown, CPU implementations, this benchmarks
suite also comes with various kernels and class sizes to choose from, according
to our needs. One important thing should be noted, however $-$ due to the
nature of the way it has been implemented, the index number of the GPU, that
will be used during tests must be specified in the configurations file during
compilation. Therefore this benchmark sets are single-GPU only and require
creation of many executable files with various configs, if the test server
has many GPUs. This problem is explained in more detail and ultimately
resolved in Chapter 4.

\subsection{Custom Deep Neural Networks model}

Last benchmark application used in this work fills the remaining gap for test
suite $-$ an multi-GPUs kernel that utilizes MPI and has potential for
multi-node training. This application was created solely for the purpose of
this paper and utilizes high-level deep learning frameworks, such as
TensorFlow and Keras, widely known and used, general-purpose programming
language such as Python, as well as Horovod~\cite{Horovod_IDRIS} framework,
that utilizes MPI and NCCL communication libraries. The mentioned components
used in this benchmark are more precisely described below:

Python is a versatile and widely-used high-level programming language known
for its simplicity, readability, and extensive standard libraries. The fact
that it is a general-purpose language means, that it can be used in various
fields, such as web development, data analysis, scientific computing and
artificial intelligence. Its high-level abstraction provides a clean and
readable syntax and the fact that its an interpreted language means that
there is no need to compile the code before running it, shortening the
time between each tests of new features. Moreover, Python comes with a large
and comprehensive standard library for basic programming tasks such as file I/O,
regular expressions, networking and data manipulation, and in case if someone
needs more specific libraries, it features an easy solution for installing
additional packages using PIP (Python Index Package).

TensorFlow is a comprehensive deep learning framework that provides low-level
and high-level APIs. While one can use TensorFlow for a wide range of machine
learning and deep learning tasks, it also includes the Keras API as a high-level
component. TensorFlow provides more low-level control, allowing developers to
customize their models and training loops extensively. This is useful for
researchers and engineers who need fine-grained control over model architecture
and training procedures.

Keras is an open-source high-level neural networks API written in Python.
It provides a user-friendly, high-level interface for building and training
neural networks. Keras was designed with simplicity and ease of use in mind.

Both TensorFlow and Keras offers extensive documentations that allows user
to quickly learn the basics of theirs modules and libraries. Moreover,
organizations such as Google~\cite{Google_Machine_Learning_Crash_Course}
or Kaggle~\cite{Kaggle} offers a practical approach in learning such concepts,
especially Kaggle comes with a wide array of many datasets and exemplary
solutions, created by the community members.

Horovod~\cite{Horovod} is an open-source distributed deep learning framework
designed to make it easier to train machine learning models on distributed
computing environments, such as clusters or cloud-based setups. It was
developed by Uber Engineering and is designed to provide efficient and
scalable distributed training for deep learning models. Horovod is
particularly well-suited for distributed training with popular deep learning
libraries like TensorFlow, PyTorch, and MXNet. Main advantages of Horovod
framework are: large scale distributed training, that allows multi-GPU and
multi-node support, efficient data parallelism, compatibility with deep
learning frameworks, such as TensorFlow and Keras, resource awarness and
dynamic scaling.

The custom deep learning benchmark, created for the purpose of this Master's
Thesis, tackles the problem of image classification. It involves training
a neural network to categorize or classify images into predefined classes
or categories. The goal is to teach the model to recognize patterns and
features in images that distinguish one category from another. This problem
is commonly addressed using supervised learning, where the neural network
learns from a labeled dataset containing examples of images and their
corresponding class labels. The dataset used has been taken from the Kaggle
website~\cite{Kaggle_Dataset} classes of the dataset represents various
species of birds, that are labeled. The birds in nearly all images occupies
over 50\% of the image, the photos themselves are of good quality $-$ no images
are blurry, done in bad lighting or misslabeled. All images are 224 $\times$
224 $\times$ 3 color images in \@.jpg format. Considering all the informations
about the dataset mentioned above, one can expect a good results when training
deep neural networks models on it. During the tests and fine-tuning phase of
the creation of the model, a very high accuracy of over 90\% has been achieved
in relatively small number of iteration. What is more important in the terms
of this work, the utilization of the GPUs and their power draw has been very
high, thus meeting the requirement for a good benchmark application, to use
possibly as much of the devices resources during measurements as possible.
Main features in this benchmark, that allowed to achieve such great results
are: usage of Horovod innate function to automatically allocate memory growth
based on chosen number of devices, Horovod handling the deep learning
distributed optimizer among the devices via callbacks, TensorFlow's
\emph{.cache\@()} and \emph{.prefetch\@()} utilizing maximum of GPUs VRAM and
finally, usage of XCeption Model $-$ a deep convolutional neural network
architecture that involves Depthwise Separable Convolutions, that has been
developed mainly for the purpose of solving the images classification problems.




