What has been achieved:
1.  New tests have been conducted: NPB-OMP have been abandoned in favor 
    of NPB-MPI. Main reason is that MPI code ensures internode communication.
2.  Tests configuration conducted:
    >  	Tests on one node, one CPU, various number of threads: (benchmark: NPB-MPI), status: [DONE]
    >  	Tests on one node, two CPUs, various number of threads: (benchmark: NPB-MPI), status: [DONE]
    >  	Tests on one node, one GPU, no changes to config so far: (benchmark: NPB-CUDA), status: [DONE]
	>	Tests on two nodes, one CPU each, various number of threads: (benchmark: NPB-MPI), status: [DONE]
		Note: This tests was performed on des01.kask and des02.kask so far
	>	Tests on one node, many GPUs, no changes to config: (benchmark: DNN), status: [W-I-P]
		Note: We can configure the 'class' size, by implementing custom number of images in train
		dataset, for example: class=A: 10,000 images and 100 classes, class=B: 20,000 images and so on.


What is planned to do:
1.  Add the preliminary tests from presentation, about
    choosing the correct CPU benchmarks as well as correct
    class sizes. (as a preliminary tests section, maybe
    shout-out to my local rig? She deserved it :3 )


What is already planned to do / questions to the supervisor:

1.  Ask about the configurations. Are they sufficient? Or we
    plan to run the tests on 2 nodes using 2 power meters
    (may require additional software such as pssh)
2.  Ask about the status of the current power meter (old one)
    and the status / plans for the second power meter.
3.  Ask the administrator of the servers to install rest
    of the necessary software (parallel-ssh in order to
    work and run benchmarks on two nodes, pip in order
    to re-install yoko-tool software for )
4.  Should I write abstract in Polish aswell?
5.  Should I write names of chapters, bibliography etc. 
    in all upper case?



Questions to the supervisors:
1.  Some benchmarks in MPI-Fortran must be run in certain configurations. Is that
	ok, that the number of processes run vary, or should they be unified somehow?

NEW QUESTIONS:
1.  Can I use 'lscpu' command instead of '/proc/cpuinfo'? for example, lscpu
    for apl15.maas (which has 2 CPUs and total of 64 logical processors) 


What to do next (conclusions after consultations):
1.  Testować i korygować wyniki, ale najważniejsze- odpalać testy

=====

EXEMPLARY MEASUREMENTS:
tests performed on sanna.kask:

Embarassingly Parallel

time mpirun -np 1 --use-hwthread-cpus --cpu-list 0 ep.D.x
time in seconds: 1510.45

time mpirun -np 10 --use-hwthread-cpus --cpu-list 0-4,10-14 ep.D.x
time in seconds: 223.03

time mpirun -np 40 --use-hwthread-cpus --cpu-list 0-39 ep.D.x
time in seconds: 57.63

=====

Lower-Upper Gauss-Seidel solver

time mpirun -np 2 --use-hwthread-cpus --cpu-list 0,10 lu.C.x
time in seconds: 322.57

time mpirun -np 16 --use-hwthread-cpus --cpu-list 0-7,10-17 lu.C.x
time in seconds: 80.06

time mpirun -np 40 --use-hwthread-cpus --cpu-list 0-39 lu.C.x
time in seconds: 36.64

=====






USEFUL COMMANDS (OMP BENCHMARKS ARE DEPRECATED, NOW WE USE MPI BENCHMARKS!!!):
taskset --cpu-list 0-15 perf stat -I 100 -e power/energy-cores/ -o test.txt ./ft.C
> taskset - makes it possible to run benchmarks pinned to certain logical processors
> --cpu-list 0-4,10-14 - parameter of taskset, pins benchmark to threads 0, 1, 2, 3 and 4
    of the first CPU and 10, 11, 12, 13 and 14 of the second CPU

> perf - linux utility for measurement of various things
> 



COMMAND TO RUN MPI FORTRAN CODE:
BE AWARE WITH USING --cpu-list "X-Y", ACCORDING TO HTOP PROCESSES ARE BOUND TO THE SAME THREADS
EVERY RUN, BUT YOU SHOULD CHECK WHICH LOGICAL PROCESSOR BELONGS TO WHICH PHYSICAL PROCESSOR AND/OR CPU:
!!!
perf stat -I 100 -e power/energy-cores/ -o test.txt mpirun --use-hwthread-cpus --cpu-list "0-7" ft.B.x
!!!



COMMAND TO RUN MPI CODE ON MANY NODES:
hostfile:
des01.kask
des02.kask

mpirun --use-hwthread-cpus --cpu-list "0-7" --mca btl tcp,self --mca orte_keep_fqdn_hostnames t --mca btl_tcp_if_include 172.20.83.0/24 --machinefile hostfile ft.B.x



CONTINUOUS PRINTING (IN THE CONSOLE) THE CPU ENERGY USED:
perf stat -a -I 100 -e power/energy-cores/






OTHER:
measurements of CPU power draw:
1. You (probably) need sudo to execute that command! (edit: only on LOCAL machine actually)
2. sudo perf stat -e power/energy-cores/,power/energy-gpu/,power/energy-pkg/ ./ft.C
3. We get energy consumed in Joules and time elapsed in seconds.

Useful link:
https://firefox-source-docs.mozilla.org/performance/perf.html


pinning the benchmark tasks to logical cores:
all cores on local machine:
time taskset --cpu-list 0-23 ./ft.C
sanna: 2 cpus, 10 threads each
time taskset --cpu-list 0-9,20-29 ./ft.C

useful links:
https://www.xmodulo.com/run-program-process-specific-cpu-cores-linux.html
https://man.archlinux.org/man/taskset.1.en

ULTIMATE COMMAND:
time sudo taskset --cpu-list 0-15 perf stat -e power/energy-cores/,power/energy-gpu/,power/energy-pkg/ ./ft.C
!!!IMPORTANT: I need to used 'sudo' only on LOCAL machine, otherwise performance counters stats are
              marked as <not supported>. On university servers, it is not necessary - it works right
              off the bat, probably due to our admin being way more knowledgable than me and adding me
              as 'sudoer' to this command (Thanks Mr. Boiński!)


ENTIRE 'yokotool.py':
import os
measurements_dir = "/home/macierz/s175405/ResearchProject12KASK/benchmark/measurements/"
power_level = open("power_level.txt","r")
test_index = open("test_index.txt","r")
used_gpus = open("used_gpus.txt","r")
os.system("yokotool /dev/usbtmc0 --pmtype wt310 read T,P -o "+measurements_dir+used_gpus.read()+"GPUS/"+power_level>



ACHTUNG!!!

Add the preliminary tests from presentation, about choosing the correct CPU benchmarks as well as correct class sizes

/proc/cpuinfo

komentarz: w tym pliku są po kolei wymieniowe logidzne procesory i w kazdym z nich będą linijki opisu
i tam bedzie taka informacja jak id core'a, 
samo sprawdzenie na htop'ie nie wystarczy, trzeba sie upewnic

czestotliwosc próbkowania
czy maximum na yokogawie 10hz jest dobre? czy może wybrać 5hz czy 2hz? porobić próby

jak częscto linux perf / intel rapl próbkuje? w przeciwienstwie to NVML'a czy yokogawy,
mamy pomiary co 0.1s, a perf daje nam calkowite zuzycie energi po wykonaniu sie benchmarku

to benchmarku na 2 node'ach KONEICZNIE aplikacja w MPI, bo pozwala działać na 2 węzłach
(są NPB-MPI), która działa na 2 wezlach i pozwala na ustalenie rozmiarów danych

finalnie trzeba bedzie raportowac średnia oraz odchylenie standardowe
(jak w artykule naukoweym)
takze 10 testów, gdzie odpalam benchmark

papiery przedluzające, dopytac sie w dziekanacie wniosek o przedluzenie (prodziekan Raczyński)
argumentować to faktem, że wyniki z tej pracy magisterskiej mogą byc wykorzystane w ew artykule naukowym

Yokogawy mogą być podłączone do apl15 oraz apl16

Dr Boiński wraca 4 września

GNU Plot to tworzenia wykresów
