What has been achieved:
1.  New tests have been conducted: NPB-OMP have been abandoned in favor 
    of NPB-MPI. Main reason is that MPI code ensures internode communication.
	UPDATE: NPB-OMP can still has its use, mainly paired with NPB-CUDA
			benchmarks: I can easily pin processes to threads using taskset,
			therefore utilizing every device in node in 100%. NPB-CUDA
			benchmarks reserves one thread and uses it in 100%, that

2.  Tests configurations conducted:
    >  	Tests on one node, one CPU, various number of threads: (benchmarks: NPB-MPI and NPB-OMP), status: [DONE]
    >  	Tests on one node, two CPUs, various number of threads: (benchmark: NPB-MPI and NPB-OMP), status: [DONE]
    >  	Tests on one node, one GPU, no changes to config so far: (benchmark: NPB-CUDA), status: [DONE]
	>	Tests on two nodes, one CPU each, various number of threads: (benchmark: NPB-MPI), status: [DONE]
		Note: This tests was performed on des01.kask and des02.kask so far
	>	Tests on one node, many GPUs, no changes to config: (benchmark: DNN), status: [DONE]
		Note: We can configure the 'class' size, by implementing custom number of images in train
		dataset, for example: class=A: 10,000 images and 100 classes, class=B: 20,000 images and so on.
	>	Tests on one node, hybrid benchmarks: one and two CPUs, various number of threads, one and more GPUs:
		(benchmarks: NPB-OMP and NPB-CUDA). Using taskset, we can pin processes to threads, utilizing entire node,
		both CPUs and GPUs to 100%.


What is planned to do:
1.  Add the preliminary tests from presentation, about
    choosing the correct CPU benchmarks as well as correct
    class sizes. (as a preliminary tests section, maybe
    shout-out to my local rig? She deserved it :3 )


What is already planned to do / questions to the supervisor:

0.  Can I create GitHub repository with all the code and
    instructions/comments for people searching for solutions
    used by me?
1.  Ask about the configurations. Are they sufficient? Or we
    plan to run the tests on 2 nodes using 2 power meters
    (may require additional software such as pssh)
2.  Ask about the status of the current power meter (old one)
    and the status / plans for the second power meter.
3.  Ask the administrator of the servers to install rest
    of the necessary software (access to apl11.maas, linux perf
	on sanna.kask, vinnana.kask and apl11.maas). 
4.	Ask the administrator how to perform tests on both sanna.kask
	and vinnana.kask (how to estabilish internode communication
	like it was with des01.kask and des02.kask)
5.  Should I write abstract in Polish aswell?


Questions to the supervisors:
1.  Some benchmarks in MPI-Fortran must be run in certain configurations. Is that
	ok, that the number of processes run vary, or should they be unified somehow?
2.  Ask Mr. Boiński about the internode communication and benchmarks between sanna and vinnana. I managed to do it between two desktops in lab 527, but those two seems to be a little different


NEW QUESTIONS:
1.  Can I use 'lscpu' command instead of '/proc/cpuinfo'? for example, lscpu
    for apl15.maas (which has 2 CPUs and total of 64 logical processors) 


What to do next (conclusions after consultations):
1.  Testować i korygować wyniki, ale najważniejsze- odpalać testy

=====================================================================

EXEMPLARY MEASUREMENTS:

Test performed on sanna.kask with CUDA compatibility = 7.5
(A100 mounted on apl11.maas has CUDA compatibility = 8.0)

// at first promising with 100% util. and very high
// power draw, but too short
bt.C = 12.19 s          avg 235~240 [W]
bt.D [unsuccesful] ~ probrably not enough VRAM
bt.E [no point of testing]

===

// overall bad - nothing happens at the beginning
cg.C [too short]
cg.D [no action useful for tests]
cg.E [no point of testing]

===

// ep.D looks very promising
ep.C [too short]
ep.D = 27.16 s          avg 155~160 [w]
ep.E = 442.79 s         ramping from 160 to 170 [W]

===

// overall bad
ft.C = 6.63 s [too short]
ft.D [unsuccesful]
ft.E [no point of testing]

===

// overall bad
is.C [too short]
is.D = 15.33 s          avg 150 [W]
is.E [not defined]

===

// lu.D looks promising
lu.C = 16.89 s          avg 180~190 [W]
lu.D = 300.74 s         fluctuates between 200~230 [W]
lu.E [unsuccesful]

===

// overall bad - nothing happens at the beginning
mg.C [too short]
mg.D [no action useful for tests]
mg.E [no point of testing]

===

// sp.D looks promising
sp.C = 10.39 s          avg 200 [W]
sp.D = 220.86 s         avg 200~210 [W]
sp.E [unsuccesful]

===

Three benchmarks chosen:
ep.D
lu.D
sp.D

=====================================================================

tests performed on sanna.kask:

Embarassingly Parallel

time mpirun -np 1 --use-hwthread-cpus --cpu-list 0 ep.D.x
time in seconds: 1510.45

time mpirun -np 10 --use-hwthread-cpus --cpu-list 0-4,10-14 ep.D.x
time in seconds: 223.03

time mpirun -np 40 --use-hwthread-cpus --cpu-list 0-39 ep.D.x
time in seconds: 57.63

=====

Lower-Upper Gauss-Seidel solver

time mpirun -np 2 --use-hwthread-cpus --cpu-list 0,10 lu.C.x
time in seconds: 322.57

time mpirun -np 16 --use-hwthread-cpus --cpu-list 0-7,10-17 lu.C.x
time in seconds: 80.06

time mpirun -np 40 --use-hwthread-cpus --cpu-list 0-39 lu.C.x
time in seconds: 36.64

=====






USEFUL COMMANDS (OMP BENCHMARKS ARE DEPRECATED, NOW WE USE MPI BENCHMARKS!!!):
taskset --cpu-list 0-15 perf stat -I 100 -e power/energy-cores/ -o test.txt ./ft.C
> taskset - makes it possible to run benchmarks pinned to certain logical processors
> --cpu-list 0-4,10-14 - parameter of taskset, pins benchmark to threads 0, 1, 2, 3 and 4
    of the first CPU and 10, 11, 12, 13 and 14 of the second CPU

> perf - linux utility for measurement of various things
> 



COMMAND TO RUN MPI FORTRAN CODE:
BE AWARE WITH USING --cpu-list "X-Y", ACCORDING TO HTOP PROCESSES ARE BOUND TO THE SAME THREADS
EVERY RUN, BUT YOU SHOULD CHECK WHICH LOGICAL PROCESSOR BELONGS TO WHICH PHYSICAL PROCESSOR AND/OR CPU:
!!!
perf stat -I 100 -e power/energy-cores/ -o test.txt mpirun --use-hwthread-cpus --cpu-list "0-7" ft.B.x
!!!



COMMAND TO RUN MPI CODE ON MANY NODES:
hostfile:
des01.kask
des02.kask

mpirun --use-hwthread-cpus --cpu-list "0-7" --mca btl tcp,self --mca orte_keep_fqdn_hostnames t --mca btl_tcp_if_include 172.20.83.0/24 --machinefile hostfile ft.B.x



CONTINUOUS PRINTING (IN THE CONSOLE) THE CPU ENERGY USED:
perf stat -a -I 100 -e power/energy-cores/






OTHER:
measurements of CPU power draw:
1. You (probably) need sudo to execute that command! (edit: only on LOCAL machine actually)
2. sudo perf stat -e power/energy-cores/,power/energy-gpu/,power/energy-pkg/ ./ft.C
3. We get energy consumed in Joules and time elapsed in seconds.

Useful link:
https://firefox-source-docs.mozilla.org/performance/perf.html


pinning the benchmark tasks to logical cores:
all cores on local machine:
time taskset --cpu-list 0-23 ./ft.C
sanna: 2 cpus, 10 threads each
time taskset --cpu-list 0-9,20-29 ./ft.C

useful links:
https://www.xmodulo.com/run-program-process-specific-cpu-cores-linux.html
https://man.archlinux.org/man/taskset.1.en

ULTIMATE COMMAND:
time sudo taskset --cpu-list 0-15 perf stat -e power/energy-cores/,power/energy-gpu/,power/energy-pkg/ ./ft.C
!!!IMPORTANT: I need to used 'sudo' only on LOCAL machine, otherwise performance counters stats are
              marked as <not supported>. On university servers, it is not necessary - it works right
              off the bat, probably due to our admin being way more knowledgable than me and adding me
              as 'sudoer' to this command (Thanks Mr. Boiński!)


ENTIRE 'yokotool.py':
import os
measurements_dir = "/home/macierz/s175405/ResearchProject12KASK/benchmark/measurements/"
power_level = open("power_level.txt","r")
test_index = open("test_index.txt","r")
used_gpus = open("used_gpus.txt","r")
os.system("yokotool /dev/usbtmc0 --pmtype wt310 read T,P -o "+measurements_dir+used_gpus.read()+"GPUS/"+power_level>



ACHTUNG!!!

Add the preliminary tests from presentation, about choosing the correct CPU benchmarks as well as correct class sizes

/proc/cpuinfo

komentarz: w tym pliku są po kolei wymieniowe logidzne procesory i w kazdym z nich będą linijki opisu
i tam bedzie taka informacja jak id core'a, 
samo sprawdzenie na htop'ie nie wystarczy, trzeba sie upewnic

czestotliwosc próbkowania
czy maximum na yokogawie 10hz jest dobre? czy może wybrać 5hz czy 2hz? porobić próby

jak częscto linux perf / intel rapl próbkuje? w przeciwienstwie to NVML'a czy yokogawy,
mamy pomiary co 0.1s, a perf daje nam calkowite zuzycie energi po wykonaniu sie benchmarku

to benchmarku na 2 node'ach KONEICZNIE aplikacja w MPI, bo pozwala działać na 2 węzłach
(są NPB-MPI), która działa na 2 wezlach i pozwala na ustalenie rozmiarów danych

finalnie trzeba bedzie raportowac średnia oraz odchylenie standardowe
(jak w artykule naukoweym)
takze 10 testów, gdzie odpalam benchmark

papiery przedluzające, dopytac sie w dziekanacie wniosek o przedluzenie (prodziekan Raczyński)
argumentować to faktem, że wyniki z tej pracy magisterskiej mogą byc wykorzystane w ew artykule naukowym

Yokogawy mogą być podłączone do apl15 oraz apl16

Dr Boiński wraca 4 września

GNU Plot to tworzenia wykresów
